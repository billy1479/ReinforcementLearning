{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTNU1mwGB1ZD"
   },
   "source": [
    "**Dependencies and imports**\n",
    "\n",
    "This can take a minute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install swig\n",
    "# !pip install --upgrade rldurham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import rldurham as rld\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJHtclV_30Re"
   },
   "source": [
    "**Reinforcement learning agent**\n",
    "\n",
    "Replace this with your own agent, I recommend starting with TD3 (lecture 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jXNHP8_U-rn"
   },
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(Agent, self).__init__()\n",
    "        \n",
    "        # Initialize networks with orthogonal weights for better gradient flow\n",
    "        self.policy = self._build_policy_network(state_dim, action_dim, hidden_dim)\n",
    "        self.value = self._build_value_network(state_dim, hidden_dim)\n",
    "        \n",
    "        # Hyperparameters tuned for stability\n",
    "        self.gamma = 0.99\n",
    "        self.learning_rate = 3e-4\n",
    "        self.gae_lambda = 0.95\n",
    "        self.clip_ratio = 0.2\n",
    "        self.entropy_coef = 0.01  # Encourage exploration\n",
    "        self.value_coef = 0.5    # Balance value and policy learning\n",
    "        \n",
    "        # Adaptive noise control\n",
    "        self.init_action_std = 0.5\n",
    "        self.action_std = self.init_action_std\n",
    "        self.action_std_decay = 0.995\n",
    "        self.min_action_std = 0.1\n",
    "        self.noise_decay_start = 100  # Start decay after 100 episodes\n",
    "        \n",
    "        # Experience management\n",
    "        self.trajectory = []\n",
    "        self.value_normalizer = RunningMeanStd()\n",
    "        self.state_normalizer = RunningMeanStd(shape=state_dim)\n",
    "        \n",
    "        # Optimization\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        # Statistics tracking\n",
    "        self.running_rewards = deque(maxlen=100)\n",
    "        self.episode_count = 0\n",
    "        self.best_reward = float('-inf')\n",
    "        self.recent_actions = deque(maxlen=5)  # For action smoothing\n",
    "        \n",
    "    def _build_policy_network(self, state_dim, action_dim, hidden_dim):\n",
    "        \"\"\"Build policy network with proper initialization\"\"\"\n",
    "        policy = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Orthogonal initialization for better training dynamics\n",
    "        for layer in policy:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                torch.nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))\n",
    "                torch.nn.init.zeros_(layer.bias)\n",
    "        \n",
    "        return policy\n",
    "    \n",
    "    def _build_value_network(self, state_dim, hidden_dim):\n",
    "        \"\"\"Build value network with proper initialization\"\"\"\n",
    "        value = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        for layer in value:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                torch.nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))\n",
    "                torch.nn.init.zeros_(layer.bias)\n",
    "        \n",
    "        return value\n",
    "    \n",
    "    def normalize_state(self, state):\n",
    "        \"\"\"Normalize state using running statistics\"\"\"\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.FloatTensor(state)\n",
    "        return torch.FloatTensor(self.state_normalizer(state.numpy()))\n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        \"\"\"Sample action with adaptive noise and temporal smoothing\"\"\"\n",
    "        state = self.normalize_state(state)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            mean_action = self.policy(state)\n",
    "            \n",
    "            # Add exploration noise with adaptive standard deviation\n",
    "            noise = torch.randn_like(mean_action) * self.action_std\n",
    "            action = torch.clamp(mean_action + noise, -1, 1)\n",
    "            \n",
    "            # Apply temporal smoothing for more natural movements\n",
    "            if len(self.recent_actions) > 0:\n",
    "                smooth_factor = 0.7\n",
    "                prev_action = np.mean([a for a in self.recent_actions], axis=0)\n",
    "                action = smooth_factor * action + (1 - smooth_factor) * torch.FloatTensor(prev_action)\n",
    "            \n",
    "            self.recent_actions.append(action.numpy())\n",
    "        \n",
    "        return action.numpy()\n",
    "    \n",
    "    def put_data(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store transition with normalized states\"\"\"\n",
    "        state = self.normalize_state(state)\n",
    "        next_state = self.normalize_state(next_state)\n",
    "        self.trajectory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Update policy and value networks with improved stability measures\"\"\"\n",
    "        if len(self.trajectory) < 1:\n",
    "            return 0, 0\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = zip(*self.trajectory)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.stack(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "        \n",
    "        # Compute normalized returns and advantages\n",
    "        with torch.no_grad():\n",
    "            values = self.value(states).squeeze()\n",
    "            next_values = self.value(next_states).squeeze()\n",
    "            \n",
    "            # Compute GAE\n",
    "            advantages = torch.zeros_like(rewards)\n",
    "            gae = 0\n",
    "            for t in reversed(range(len(rewards))):\n",
    "                if t == len(rewards) - 1:\n",
    "                    next_value = next_values[t]\n",
    "                else:\n",
    "                    next_value = values[t + 1]\n",
    "                \n",
    "                delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]\n",
    "                gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae\n",
    "                advantages[t] = gae\n",
    "            \n",
    "            returns = advantages + values\n",
    "            \n",
    "            # Normalize advantages\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Compute action probabilities\n",
    "        mean_actions = self.policy(states)\n",
    "        dist = torch.distributions.Normal(mean_actions, self.action_std)\n",
    "        old_log_probs = dist.log_prob(actions).sum(dim=1)\n",
    "        \n",
    "        # Multiple epochs of optimization for better sample efficiency\n",
    "        for _ in range(4):\n",
    "            # Compute new action probabilities\n",
    "            mean_actions = self.policy(states)\n",
    "            dist = torch.distributions.Normal(mean_actions, self.action_std)\n",
    "            new_log_probs = dist.log_prob(actions).sum(dim=1)\n",
    "            \n",
    "            # Compute entropy for exploration\n",
    "            entropy = dist.entropy().mean()\n",
    "            \n",
    "            # Compute policy loss with clipping\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs.detach())\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # Compute value loss\n",
    "            value_pred = self.value(states).squeeze()\n",
    "            value_loss = F.mse_loss(value_pred, returns.detach())\n",
    "            \n",
    "            # Combined loss with entropy bonus\n",
    "            loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy\n",
    "            \n",
    "            # Optimize\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=0.5)\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        # Clear trajectory buffer\n",
    "        self.trajectory = []\n",
    "        \n",
    "        # Update exploration noise\n",
    "        if self.episode_count > self.noise_decay_start:\n",
    "            self.action_std = max(self.min_action_std, \n",
    "                                self.action_std * self.action_std_decay)\n",
    "        \n",
    "        return policy_loss.item(), value_loss.item()\n",
    "    \n",
    "    def update_stats(self, episode_reward):\n",
    "        \"\"\"Update running statistics and episode count\"\"\"\n",
    "        self.running_rewards.append(episode_reward)\n",
    "        self.episode_count += 1\n",
    "        self.best_reward = max(self.best_reward, episode_reward)\n",
    "    \n",
    "    def get_average_reward(self):\n",
    "        \"\"\"Calculate average reward over last 100 episodes\"\"\"\n",
    "        return np.mean(self.running_rewards) if self.running_rewards else 0\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset temporal smoothing between episodes\"\"\"\n",
    "        self.recent_actions.clear()\n",
    "\n",
    "class RunningMeanStd:\n",
    "    \"\"\"Tracks running mean and standard deviation for normalization\"\"\"\n",
    "    def __init__(self, shape=(), epsilon=1e-4):\n",
    "        self.mean = np.zeros(shape, dtype=np.float32)\n",
    "        self.var = np.ones(shape, dtype=np.float32)\n",
    "        self.count = epsilon\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_var = np.var(x, axis=0)\n",
    "        batch_count = x.shape[0] if len(x.shape) > 1 else 1\n",
    "        \n",
    "        delta = batch_mean - self.mean\n",
    "        self.mean += delta * batch_count / (self.count + batch_count)\n",
    "        m_a = self.var * self.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / (self.count + batch_count)\n",
    "        self.var = M2 / (self.count + batch_count)\n",
    "        self.count += batch_count\n",
    "        \n",
    "        return (x - self.mean) / np.sqrt(self.var + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEv4ZjXmyrHo"
   },
   "source": [
    "**Prepare the environment and wrap it to capture statistics, logs, and videos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Xrcek4hxDXl"
   },
   "outputs": [],
   "source": [
    "env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\")\n",
    "# env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\", hardcore=True) # only attempt this when your agent has solved the non-hardcore version\n",
    "\n",
    "# get statistics, logs, and videos\n",
    "env = rld.Recorder(\n",
    "    env,\n",
    "    smoothing=10,                       # track rolling averages (useful for plotting)\n",
    "    video=True,                         # enable recording videos\n",
    "    video_folder=\"videos\",              # folder for videos\n",
    "    video_prefix=\"xxxx00-agent-video\",  # prefix for videos (replace xxxx00 with your username)\n",
    "    logs=True,                          # keep logs\n",
    ")\n",
    "\n",
    "# training on CPU recommended\n",
    "rld.check_device()\n",
    "\n",
    "# environment info\n",
    "discrete_act, discrete_obs, act_dim, obs_dim = rld.env_info(env, print_out=True)\n",
    "\n",
    "# render start image\n",
    "env.reset(seed=42)\n",
    "rld.render(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "rDl6ViIDlVOk",
    "outputId": "731e4ce7-c98d-4bde-8a2c-fbdf1410e24f"
   },
   "outputs": [],
   "source": [
    "# in the submission please use seed_everything with seed 42 for verification\n",
    "seed, observation, info = rld.seed_everything(42, env)\n",
    "\n",
    "# initialise agent\n",
    "agent = Agent(env.observation_space.shape[0], env.action_space.shape[0])\n",
    "max_episodes = 1000\n",
    "max_timesteps = 2000\n",
    "\n",
    "# track statistics for plotting\n",
    "tracker = rld.InfoTracker()\n",
    "\n",
    "# switch video recording off (only switch on every x episodes as this is slow)\n",
    "env.video = False\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    # Set up episode\n",
    "    env.info = episode % 10 == 0\n",
    "    env.video = episode % 10 == 0\n",
    "    current_observation, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    # Reset agent's temporal smoothing\n",
    "    agent.reset()\n",
    "    \n",
    "    # Run episode\n",
    "    for t in range(max_timesteps):\n",
    "        # Get action from agent\n",
    "        action = agent.sample_action(current_observation)\n",
    "        \n",
    "        # Take step in environment\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Store transition and update episode reward\n",
    "        agent.put_data(current_observation, action, reward, next_observation, \n",
    "                      terminated or truncated)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # Update observation\n",
    "        current_observation = next_observation\n",
    "        \n",
    "        # Check if episode is done\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    # Train agent and update statistics\n",
    "    policy_loss, value_loss = agent.train()\n",
    "    agent.update_stats(episode_reward)\n",
    "    \n",
    "    # Print progress every 10 episodes\n",
    "    if episode % 10 == 0:\n",
    "        avg_reward = agent.get_average_reward()\n",
    "        print(f\"Episode {episode+1}, Reward: {episode_reward:.2f}, \"\n",
    "              f\"Avg Reward: {avg_reward:.2f}, \"\n",
    "              f\"Action STD: {agent.action_std:.3f}\")\n",
    "    \n",
    "    # Track and plot\n",
    "    tracker.track(info)\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        tracker.plot(r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Write log file\n",
    "env.write_log(folder=\"logs\", file=\"xxxx00-agent-log.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small demo with a predefined heuristic that is suboptimal and has no notion of balance (and is designed for the orignal BipedalWalker environment)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.box2d.bipedal_walker import BipedalWalkerHeuristics\n",
    "\n",
    "env = rld.make(\n",
    "    \"rldurham/Walker\",\n",
    "    # \"BipedalWalker-v3\",\n",
    "    render_mode=\"human\",\n",
    "    # render_mode=\"rgb_array\",\n",
    "    hardcore=False,\n",
    "    # hardcore=True,\n",
    ")\n",
    "_, obs, info = rld.seed_everything(42, env)\n",
    "\n",
    "heuristics = BipedalWalkerHeuristics()\n",
    "\n",
    "act = heuristics.step_heuristic(obs)\n",
    "for _ in range(500):\n",
    "    obs, rew, terminated, truncated, info = env.step(act)\n",
    "    act = heuristics.step_heuristic(obs)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    if env.render_mode == \"rgb_array\":\n",
    "        rld.render(env, clear=True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
