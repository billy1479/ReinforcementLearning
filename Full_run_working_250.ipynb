{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTNU1mwGB1ZD"
   },
   "source": [
    "**Dependencies and imports**\n",
    "\n",
    "This can take a minute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install swig\n",
    "# !pip install --upgrade rldurham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import rldurham as rld\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJHtclV_30Re"
   },
   "source": [
    "## Reinforcement Learning set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jXNHP8_U-rn"
   },
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(Agent, self).__init__()\n",
    "        \n",
    "        # Initialize networks with orthogonal weights for better gradient flow\n",
    "        self.policy = self._build_policy_network(state_dim, action_dim, hidden_dim)\n",
    "        self.value = self._build_value_network(state_dim, hidden_dim)\n",
    "        \n",
    "        # Hyperparameters tuned for stability\n",
    "        self.gamma = 0.99\n",
    "        self.learning_rate = 3e-4 #4e-4 was good\n",
    "        self.gae_lambda = 0.95\n",
    "        self.clip_ratio = 0.2\n",
    "        self.entropy_coef = 0.01  # Encourage exploration\n",
    "        self.value_coef = 0.5    # Balance value and policy learning\n",
    "        \n",
    "        # Adaptive noise control\n",
    "        self.init_action_std = 0.6\n",
    "        self.action_std = self.init_action_std\n",
    "        self.action_std_decay = 0.999\n",
    "        self.min_action_std = 0.2\n",
    "        self.noise_decay_start = 1000  # Start decay after 100 episodes # 500 was good\n",
    "        \n",
    "        # Experience management\n",
    "        self.trajectory = []\n",
    "        self.experience_buffer = []  # Store successful episodes\n",
    "        self.buffer_size = 10000     # Maximum buffer size\n",
    "        self.success_threshold = 50  # Threshold to consider an episode \"successful\"\n",
    "        self.replay_ratio = 0.3  \n",
    "        \n",
    "        self.value_normalizer = RunningMeanStd()\n",
    "        self.state_normalizer = RunningMeanStd(shape=state_dim)\n",
    "        self.reward_normalizer = RunningMeanStd()\n",
    "        \n",
    "        # Optimization\n",
    "        self.policy_optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        self.value_optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        self.snapshot_interval = 100  # Save model snapshot every N episodes\n",
    "        self.snapshots = []\n",
    "        self.max_snapshots = 3\n",
    "        self.snapshot_weights = [0.7, 0.2, 0.1]  # Weights for ensemble predictions\n",
    "        \n",
    "        # Statistics tracking\n",
    "        self.running_rewards = deque(maxlen=100)\n",
    "        self.early_stopping_patience = 50\n",
    "        self.patience_counter = 0\n",
    "        self.episode_count = 0\n",
    "        self.best_reward = float('-inf')\n",
    "        self.best_avg_reward = float('-inf')\n",
    "        self.recent_actions = deque(maxlen=5)  # For action smoothing\n",
    "        \n",
    "    def _build_policy_network(self, state_dim, action_dim, hidden_dim):\n",
    "        \"\"\"Build policy network with proper initialization\"\"\"\n",
    "        policy = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Orthogonal initialization for better training dynamics\n",
    "        for layer in policy:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                torch.nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))\n",
    "                torch.nn.init.zeros_(layer.bias)\n",
    "        \n",
    "        return policy\n",
    "    \n",
    "    def _build_value_network(self, state_dim, hidden_dim):\n",
    "        \"\"\"Build value network with proper initialization\"\"\"\n",
    "        value = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        for layer in value:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                torch.nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))\n",
    "                torch.nn.init.zeros_(layer.bias)\n",
    "        \n",
    "        return value\n",
    "    \n",
    "    def normalize_state(self, state):\n",
    "        \"\"\"Normalize state using running statistics\"\"\"\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.FloatTensor(state)\n",
    "        return torch.FloatTensor(self.state_normalizer(state.numpy()))\n",
    "    \n",
    "    def normalize_reward(self, reward):\n",
    "        \"\"\"Normalize rewards for more stable learning\"\"\"\n",
    "        self.reward_normalizer(np.array([reward]))\n",
    "        return reward\n",
    "    \n",
    "    def sample_action(self, state, deterministic=False):\n",
    "        \"\"\"Sample action with adaptive noise and temporal smoothing\"\"\"\n",
    "        state = self.normalize_state(state)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            mean_action = self.policy(state)\n",
    "            \n",
    "            if deterministic:\n",
    "                action = mean_action\n",
    "            else:\n",
    "                # Add exploration noise with adaptive standard deviation\n",
    "                noise = torch.randn_like(mean_action) * self.action_std\n",
    "                action = torch.clamp(mean_action + noise, -1, 1)\n",
    "                \n",
    "                # Apply temporal smoothing for more natural movements\n",
    "                if len(self.recent_actions) > 0:\n",
    "                    smooth_factor = 0.7\n",
    "                    prev_action = np.mean([a for a in self.recent_actions], axis=0)\n",
    "                    action = smooth_factor * action + (1 - smooth_factor) * torch.FloatTensor(prev_action)\n",
    "                \n",
    "                # Ensemble prediction from snapshots for more robustness\n",
    "                if len(self.snapshots) > 0 and np.random.random() < 0.3:  # 30% chance to use ensemble\n",
    "                    ensemble_actions = [snapshot_policy(state) for snapshot_policy, _ in self.snapshots]\n",
    "                    ensemble_actions.append(action)  # Include current policy\n",
    "                    \n",
    "                    # Weight the actions based on recency\n",
    "                    weights = self.snapshot_weights[:len(self.snapshots)] + [1.0]\n",
    "                    weights = [w/sum(weights) for w in weights]\n",
    "                    \n",
    "                    action = sum(w * a for w, a in zip(weights, ensemble_actions))\n",
    "                    action = torch.clamp(action, -1, 1)\n",
    "            \n",
    "            self.recent_actions.append(action.numpy())\n",
    "        \n",
    "        return action.numpy()\n",
    "    \n",
    "    def put_data(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store transition with normalized states\"\"\"\n",
    "        state = self.normalize_state(state)\n",
    "        next_state = self.normalize_state(next_state)\n",
    "        norm_reward = self.normalize_reward(reward)\n",
    "        self.trajectory.append((state, action, norm_reward, next_state, done))\n",
    "    \n",
    "    def add_to_experience_buffer(self, episode_data, episode_reward):\n",
    "        \"\"\"Add successful episodes to the experience buffer\"\"\"\n",
    "        if episode_reward > self.success_threshold:\n",
    "            self.experience_buffer.extend(episode_data)\n",
    "            # Trim buffer if it gets too large\n",
    "            if len(self.experience_buffer) > self.buffer_size:\n",
    "                excess = len(self.experience_buffer) - self.buffer_size\n",
    "                self.experience_buffer = self.experience_buffer[excess:]\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Update policy and value networks with improved stability measures\"\"\"\n",
    "        if len(self.trajectory) < 1:\n",
    "            return 0, 0\n",
    "        \n",
    "        # Check if we should add the trajectory to the experience buffer\n",
    "        episode_reward = sum([r for _, _, r, _, _ in self.trajectory])\n",
    "        self.add_to_experience_buffer(self.trajectory, episode_reward)\n",
    "        \n",
    "        # Mix in some past successful experiences if available\n",
    "        if len(self.experience_buffer) > 0 and np.random.random() < self.replay_ratio:\n",
    "            replay_size = min(len(self.experience_buffer), int(len(self.trajectory) * 0.5))\n",
    "            replay_samples = random.sample(self.experience_buffer, replay_size)\n",
    "            training_data = self.trajectory + replay_samples\n",
    "        else:\n",
    "            training_data = self.trajectory\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = zip(*training_data)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.stack(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "        \n",
    "        # Compute normalized returns and advantages\n",
    "        with torch.no_grad():\n",
    "            values = self.value(states).squeeze()\n",
    "            next_values = self.value(next_states).squeeze()\n",
    "            \n",
    "            # Compute GAE\n",
    "            advantages = torch.zeros_like(rewards)\n",
    "            gae = 0\n",
    "            for t in reversed(range(len(rewards))):\n",
    "                if t == len(rewards) - 1:\n",
    "                    next_value = next_values[t]\n",
    "                else:\n",
    "                    next_value = values[t + 1]\n",
    "                \n",
    "                delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]\n",
    "                gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae\n",
    "                advantages[t] = gae\n",
    "            \n",
    "            returns = advantages + values\n",
    "            \n",
    "            # Normalize advantages\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Compute action probabilities\n",
    "        mean_actions = self.policy(states)\n",
    "        dist = torch.distributions.Normal(mean_actions, self.action_std)\n",
    "        old_log_probs = dist.log_prob(actions).sum(dim=1)\n",
    "        \n",
    "        # Multiple epochs of optimization with early stopping\n",
    "        policy_losses = []\n",
    "        value_losses = []\n",
    "        kl_divs = []\n",
    "        \n",
    "        for epoch in range(10):  # 10 epochs max\n",
    "            # Compute new action probabilities\n",
    "            mean_actions = self.policy(states)\n",
    "            dist = torch.distributions.Normal(mean_actions, self.action_std)\n",
    "            new_log_probs = dist.log_prob(actions).sum(dim=1)\n",
    "            \n",
    "            # Compute entropy for exploration\n",
    "            entropy = dist.entropy().mean()\n",
    "            \n",
    "            # Compute policy loss with clipping\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs.detach())\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # Compute value loss with clipping\n",
    "            value_pred = self.value(states).squeeze()\n",
    "            value_clipped = values + torch.clamp(value_pred - values, -self.clip_ratio, self.clip_ratio)\n",
    "            value_loss_1 = F.mse_loss(value_pred, returns.detach())\n",
    "            value_loss_2 = F.mse_loss(value_clipped, returns.detach())\n",
    "            value_loss = torch.max(value_loss_1, value_loss_2)\n",
    "            \n",
    "            # Compute KL divergence for early stopping\n",
    "            approx_kl = ((old_log_probs - new_log_probs) * ratio).mean().item()\n",
    "            kl_divs.append(approx_kl)\n",
    "            \n",
    "            # Store losses\n",
    "            policy_losses.append(policy_loss.item())\n",
    "            value_losses.append(value_loss.item())\n",
    "            \n",
    "            # Early stopping based on KL divergence\n",
    "            if approx_kl > 0.015:\n",
    "                break\n",
    "                \n",
    "            # Update policy network\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss_with_entropy = policy_loss - self.entropy_coef * entropy\n",
    "            policy_loss_with_entropy.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), max_norm=0.5)\n",
    "            self.policy_optimizer.step()\n",
    "            \n",
    "            # Update value network separately for more stability\n",
    "            self.value_optimizer.zero_grad()\n",
    "            (self.value_coef * value_loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.value.parameters(), max_norm=0.5)\n",
    "            self.value_optimizer.step()\n",
    "        \n",
    "        # Clear trajectory buffer\n",
    "        self.trajectory = []\n",
    "        \n",
    "        # Update exploration noise\n",
    "        if self.episode_count > self.noise_decay_start:\n",
    "            self.action_std = max(self.min_action_std, \n",
    "                                self.action_std * self.action_std_decay)\n",
    "        \n",
    "        return np.mean(policy_losses), np.mean(value_losses)\n",
    "    \n",
    "    def update_stats(self, episode_reward):\n",
    "        \"\"\"Update running statistics and episode count\"\"\"\n",
    "        self.running_rewards.append(episode_reward)\n",
    "        self.episode_count += 1\n",
    "        self.best_reward = max(self.best_reward, episode_reward)\n",
    "        \n",
    "        # Early stopping check\n",
    "        current_avg_reward = self.get_average_reward()\n",
    "        if current_avg_reward > self.best_avg_reward:\n",
    "            self.best_avg_reward = current_avg_reward\n",
    "            self.patience_counter = 0\n",
    "            \n",
    "            # Save model snapshot for ensemble\n",
    "            if self.episode_count % self.snapshot_interval == 0:\n",
    "                policy_copy = copy.deepcopy(self.policy)\n",
    "                value_copy = copy.deepcopy(self.value)\n",
    "                self.snapshots.append((policy_copy, value_copy))\n",
    "                if len(self.snapshots) > self.max_snapshots:\n",
    "                    self.snapshots.pop(0)  # Remove oldest snapshot\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "            \n",
    "            # If patience exceeded, revert to best snapshot\n",
    "            if self.patience_counter >= self.early_stopping_patience and len(self.snapshots) > 0:\n",
    "                print(f\"Performance plateaued for {self.early_stopping_patience} episodes. Reverting to previous best model.\")\n",
    "                latest_snapshot = self.snapshots[-1]\n",
    "                self.policy = copy.deepcopy(latest_snapshot[0])\n",
    "                self.value = copy.deepcopy(latest_snapshot[1])\n",
    "                self.patience_counter = 0\n",
    "    \n",
    "    def get_average_reward(self):\n",
    "        \"\"\"Calculate average reward over last 100 episodes\"\"\"\n",
    "        return np.mean(self.running_rewards) if self.running_rewards else 0\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset temporal smoothing between episodes\"\"\"\n",
    "        self.recent_actions.clear()\n",
    "        \n",
    "    def save(self, filename):\n",
    "        \"\"\"\n",
    "        Save model state with enhanced security and compatibility.\n",
    "        This version handles separate optimizers for policy and value networks.\n",
    "        \"\"\"\n",
    "        # Save neural network states and their optimizers separately\n",
    "        network_state = {\n",
    "            'policy_state_dict': self.policy.state_dict(),\n",
    "            'value_state_dict': self.value.state_dict(),\n",
    "            'policy_optimizer_state_dict': self.policy_optimizer.state_dict(),\n",
    "            'value_optimizer_state_dict': self.value_optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(network_state, filename + '_networks.pt')\n",
    "        \n",
    "        # Save normalizer statistics as plain numbers\n",
    "        normalizer_state = {\n",
    "            'state_mean': self.state_normalizer.mean.tolist(),\n",
    "            'state_var': self.state_normalizer.var.tolist(),\n",
    "            'state_count': float(self.state_normalizer.count),\n",
    "            'value_mean': self.value_normalizer.mean.tolist(),\n",
    "            'value_var': self.value_normalizer.var.tolist(),\n",
    "            'value_count': float(self.value_normalizer.count),\n",
    "        }\n",
    "        \n",
    "        # Save other parameters\n",
    "        other_state = {\n",
    "            'episode_count': self.episode_count,\n",
    "            'best_reward': float(self.best_reward),\n",
    "            'action_std': float(self.action_std),\n",
    "            'normalizer_state': normalizer_state\n",
    "        }\n",
    "        \n",
    "        # Save as JSON for better compatibility\n",
    "        with open(filename + '_other.json', 'w') as f:\n",
    "            json.dump(other_state, f)\n",
    "\n",
    "    def load(self, filename):\n",
    "        \"\"\"\n",
    "        Load model state with enhanced security and compatibility.\n",
    "        This version handles separate optimizers for policy and value networks.\n",
    "        \"\"\"\n",
    "        # Load neural network states\n",
    "        network_state = torch.load(filename + '_networks.pt', weights_only=True)\n",
    "        self.policy.load_state_dict(network_state['policy_state_dict'])\n",
    "        self.value.load_state_dict(network_state['value_state_dict'])\n",
    "        self.policy_optimizer.load_state_dict(network_state['policy_optimizer_state_dict'])\n",
    "        self.value_optimizer.load_state_dict(network_state['value_optimizer_state_dict'])\n",
    "        \n",
    "        # Load other parameters from JSON\n",
    "        try:\n",
    "            with open(filename + '_other.json', 'r') as f:\n",
    "                other_state = json.load(f)\n",
    "                \n",
    "            # Restore normalizer states\n",
    "            normalizer_state = other_state['normalizer_state']\n",
    "            \n",
    "            # Reconstruct state normalizer\n",
    "            self.state_normalizer.mean = np.array(normalizer_state['state_mean'], dtype=np.float32)\n",
    "            self.state_normalizer.var = np.array(normalizer_state['state_var'], dtype=np.float32)\n",
    "            self.state_normalizer.count = normalizer_state['state_count']\n",
    "            \n",
    "            # Reconstruct value normalizer\n",
    "            self.value_normalizer.mean = np.array(normalizer_state['value_mean'], dtype=np.float32)\n",
    "            self.value_normalizer.var = np.array(normalizer_state['value_var'], dtype=np.float32)\n",
    "            self.value_normalizer.count = normalizer_state['value_count']\n",
    "            \n",
    "            # Restore other parameters\n",
    "            self.episode_count = other_state['episode_count']\n",
    "            self.best_reward = other_state['best_reward']\n",
    "            self.action_std = other_state['action_std']\n",
    "            \n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "            print(f\"Warning: Could not load complete state. Only network weights were restored. Error: {e}\")\n",
    "\n",
    "class RunningMeanStd:\n",
    "    \"\"\"Tracks running mean and standard deviation for normalization\"\"\"\n",
    "    def __init__(self, shape=(), epsilon=1e-4):\n",
    "        self.mean = np.zeros(shape, dtype=np.float32)\n",
    "        self.var = np.ones(shape, dtype=np.float32)\n",
    "        self.count = epsilon\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_var = np.var(x, axis=0)\n",
    "        batch_count = x.shape[0] if len(x.shape) > 1 else 1\n",
    "        \n",
    "        delta = batch_mean - self.mean\n",
    "        self.mean += delta * batch_count / (self.count + batch_count)\n",
    "        m_a = self.var * self.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / (self.count + batch_count)\n",
    "        self.var = M2 / (self.count + batch_count)\n",
    "        self.count += batch_count\n",
    "        \n",
    "        return (x - self.mean) / np.sqrt(self.var + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEv4ZjXmyrHo"
   },
   "source": [
    "### Prepare the environment and wrap it to capture statistics, logs, and video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Xrcek4hxDXl"
   },
   "outputs": [],
   "source": [
    "env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\")\n",
    "# env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\", hardcore=True) # only attempt this when your agent has solved the non-hardcore version\n",
    "\n",
    "# get statistics, logs, and videos\n",
    "env = rld.Recorder(\n",
    "    env,\n",
    "    smoothing=10,                       # track rolling averages (useful for plotting)\n",
    "    video=True,                         # enable recording videos\n",
    "    video_folder=\"videos\",              # folder for videos\n",
    "    video_prefix=\"xxxx00-agent-video\",  # prefix for videos (replace xxxx00 with your username)\n",
    "    logs=True,                          # keep logs\n",
    ")\n",
    "\n",
    "# training on CPU recommended\n",
    "rld.check_device()\n",
    "\n",
    "# environment info\n",
    "discrete_act, discrete_obs, act_dim, obs_dim = rld.env_info(env, print_out=True)\n",
    "\n",
    "# render start image\n",
    "env.reset(seed=42)\n",
    "rld.render(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the submission please use seed_everything with seed 42 for verification\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Ensure reproducibility across runs\n",
    "seed, observation, info = rld.seed_everything(42, env)\n",
    "\n",
    "# Initialize agent\n",
    "agent = Agent(env.observation_space.shape[0], env.action_space.shape[0])\n",
    "max_episodes = 20000\n",
    "max_timesteps = 2000\n",
    "\n",
    "# Track statistics for plotting with enhanced metrics\n",
    "tracker = rld.InfoTracker()\n",
    "evaluation_interval = 50  # Evaluate agent without exploration every N episodes\n",
    "checkpoint_interval = 500  # Save model checkpoint every N episodes\n",
    "early_stop_patience = 200  # Episodes to wait before early stopping\n",
    "best_avg_reward = float('-inf')\n",
    "patience_counter = 0\n",
    "min_episodes_before_early_stop = 1000  # Minimum training episodes before considering early stopping\n",
    "\n",
    "# Create directories for checkpoints and metrics\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "metrics_dir = \"metrics\"\n",
    "plots_dir = os.path.join(metrics_dir, \"plots\")\n",
    "csv_dir = os.path.join(metrics_dir, \"csv\")\n",
    "video_dir = os.path.join(metrics_dir, \"videos\")\n",
    "\n",
    "# Create all necessary directories\n",
    "for directory in [checkpoint_dir, metrics_dir, plots_dir, csv_dir, video_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Configure video path if the environment supports it\n",
    "try:\n",
    "    env.set_video_path(video_dir)\n",
    "except:\n",
    "    print(f\"Environment doesn't support custom video paths. Videos may be saved elsewhere.\")\n",
    "\n",
    "# Initialize training metrics tracking\n",
    "training_stats = {\n",
    "    'episode_rewards': [],\n",
    "    'avg_rewards': [],\n",
    "    'policy_losses': [],\n",
    "    'value_losses': [],\n",
    "    'episode_lengths': [],\n",
    "    'learning_rates': []\n",
    "}\n",
    "\n",
    "# Vectorize environment operations if possible\n",
    "# Note: This is a placeholder - actual implementation depends on the env type\n",
    "try:\n",
    "    env = rld.make_env_faster(env)  # Fictional function - replace with actual vectorization if available\n",
    "except:\n",
    "    print(\"Environment vectorization not available, using standard environment\")\n",
    "\n",
    "# Learning rate scheduler for gradual LR reduction\n",
    "initial_lr = agent.learning_rate\n",
    "min_lr = initial_lr / 10\n",
    "lr_decay_factor = 0.995\n",
    "lr_decay_start = 1000\n",
    "\n",
    "# Start training timer\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    # Set up episode - only record video occasionally to save time\n",
    "    is_evaluation_episode = episode % evaluation_interval == 0\n",
    "    env.info = episode % 10 == 0\n",
    "    env.video = episode % 250 == 0  # Reduced frequency for video recording\n",
    "    \n",
    "    current_observation, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_start_time = time.time()\n",
    "    \n",
    "    # Reset agent's temporal smoothing\n",
    "    agent.reset()\n",
    "    \n",
    "    # Store transitions for batch processing\n",
    "    episode_transitions = []\n",
    "    \n",
    "    # Run episode\n",
    "    for t in range(max_timesteps):\n",
    "        # Get action from agent (deterministic for evaluation episodes)\n",
    "        action = agent.sample_action(current_observation, deterministic=is_evaluation_episode)\n",
    "        \n",
    "        # Take step in environment\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Store transition\n",
    "        if not is_evaluation_episode:  # Don't store evaluation episodes\n",
    "            agent.put_data(current_observation, action, reward, next_observation, \n",
    "                          terminated or truncated)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        episode_transitions.append((current_observation, action, reward, next_observation, \n",
    "                                   terminated or truncated))\n",
    "        \n",
    "        # Update observation\n",
    "        current_observation = next_observation\n",
    "        \n",
    "        # Check if episode is done\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    # Only update the agent if this wasn't an evaluation episode\n",
    "    if not is_evaluation_episode:\n",
    "        # Update learning rate according to schedule\n",
    "        if episode > lr_decay_start:\n",
    "            new_lr = max(min_lr, initial_lr * (lr_decay_factor ** (episode - lr_decay_start)))\n",
    "            for param_group in agent.policy_optimizer.param_groups:\n",
    "                param_group['lr'] = new_lr\n",
    "            for param_group in agent.value_optimizer.param_groups:\n",
    "                param_group['lr'] = new_lr\n",
    "            agent.learning_rate = new_lr\n",
    "            \n",
    "        # Train agent in batches (more efficient)\n",
    "        policy_loss, value_loss = agent.train()\n",
    "    else:\n",
    "        policy_loss, value_loss = 0, 0  # No training during evaluation\n",
    "    \n",
    "    # Update agent statistics\n",
    "    agent.update_stats(episode_reward)\n",
    "    \n",
    "    # Save metrics\n",
    "    episode_length = t + 1\n",
    "    training_stats['episode_rewards'].append(episode_reward)\n",
    "    training_stats['avg_rewards'].append(agent.get_average_reward())\n",
    "    training_stats['policy_losses'].append(policy_loss)\n",
    "    training_stats['value_losses'].append(value_loss)\n",
    "    training_stats['episode_lengths'].append(episode_length)\n",
    "    training_stats['learning_rates'].append(agent.learning_rate)\n",
    "    \n",
    "    # Save metrics to CSV after each episode\n",
    "    if episode == 0:\n",
    "        # Create CSV files with headers\n",
    "        metrics_files = {\n",
    "            'rewards': os.path.join(csv_dir, 'episode_rewards.csv'),\n",
    "            'avg_rewards': os.path.join(csv_dir, 'avg_rewards.csv'),\n",
    "            'losses': os.path.join(csv_dir, 'losses.csv'),\n",
    "            'episode_data': os.path.join(csv_dir, 'episode_data.csv')\n",
    "        }\n",
    "        \n",
    "        # Initialize CSV files with headers\n",
    "        with open(metrics_files['rewards'], 'w') as f:\n",
    "            f.write('episode,reward\\n')\n",
    "        \n",
    "        with open(metrics_files['avg_rewards'], 'w') as f:\n",
    "            f.write('episode,avg_reward_last_100\\n')\n",
    "            \n",
    "        with open(metrics_files['losses'], 'w') as f:\n",
    "            f.write('episode,policy_loss,value_loss\\n')\n",
    "            \n",
    "        with open(metrics_files['episode_data'], 'w') as f:\n",
    "            f.write('episode,length,learning_rate,action_std\\n')\n",
    "    \n",
    "    # Append data to CSV files\n",
    "    with open(os.path.join(csv_dir, 'episode_rewards.csv'), 'a') as f:\n",
    "        f.write(f\"{episode},{episode_reward}\\n\")\n",
    "        \n",
    "    with open(os.path.join(csv_dir, 'avg_rewards.csv'), 'a') as f:\n",
    "        f.write(f\"{episode},{agent.get_average_reward()}\\n\")\n",
    "        \n",
    "    with open(os.path.join(csv_dir, 'losses.csv'), 'a') as f:\n",
    "        f.write(f\"{episode},{policy_loss},{value_loss}\\n\")\n",
    "        \n",
    "    with open(os.path.join(csv_dir, 'episode_data.csv'), 'a') as f:\n",
    "        f.write(f\"{episode},{episode_length},{agent.learning_rate},{agent.action_std}\\n\")\n",
    "    \n",
    "    # Print progress with more detailed information\n",
    "    if episode % 10 == 0:\n",
    "        avg_reward = agent.get_average_reward()\n",
    "        elapsed_time = time.time() - start_time\n",
    "        eps_per_second = (episode + 1) / elapsed_time if elapsed_time > 0 else 0\n",
    "        estimated_remaining = (max_episodes - episode - 1) / eps_per_second / 60 if eps_per_second > 0 else 0\n",
    "        \n",
    "        print(f\"Episode {episode+1}/{max_episodes}, Reward: {episode_reward:.2f}, \"\n",
    "              f\"Avg Reward: {avg_reward:.2f}, Action STD: {agent.action_std:.3f}, \"\n",
    "              f\"Episode Length: {episode_length}, Time: {elapsed_time:.1f}s, \"\n",
    "              f\"Est. Remaining: {estimated_remaining:.1f}min\")\n",
    "    \n",
    "    # Track and plot\n",
    "    tracker.track(info)\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        tracker.plot(r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "        \n",
    "        # Add additional plots for training metrics\n",
    "        # Create plot filename with timestamp for easier organization\n",
    "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        plot_filename = os.path.join(plots_dir, f\"metrics_ep{episode+1}_{timestamp}.png\")\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.plot(training_stats['episode_rewards'][-100:])\n",
    "        plt.title('Recent Episode Rewards')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.plot(training_stats['policy_losses'][-100:], label='Policy Loss')\n",
    "        plt.plot(training_stats['value_losses'][-100:], label='Value Loss')\n",
    "        plt.title('Losses')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 3, 3)\n",
    "        plt.plot(training_stats['episode_lengths'][-100:])\n",
    "        plt.title('Episode Lengths')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Steps')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 3, 4)\n",
    "        plt.plot(training_stats['learning_rates'])\n",
    "        plt.title('Learning Rate')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('LR')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 3, 5)\n",
    "        # Moving average of rewards for smoothed trend\n",
    "        window_size = min(25, len(training_stats['episode_rewards']))\n",
    "        if window_size > 1:\n",
    "            smoothed_rewards = np.convolve(\n",
    "                training_stats['episode_rewards'], \n",
    "                np.ones(window_size)/window_size, \n",
    "                mode='valid'\n",
    "            )\n",
    "            plt.plot(smoothed_rewards)\n",
    "            plt.title(f'Smoothed Rewards (window={window_size})')\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Smoothed Reward')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 3, 6)\n",
    "        # Compare action standard deviation with episode rewards\n",
    "        recent_episodes = min(100, len(training_stats['episode_rewards']))\n",
    "        if recent_episodes > 0:\n",
    "            episodes_x = list(range(len(training_stats['episode_rewards'])))[-recent_episodes:]\n",
    "            ax1 = plt.gca()\n",
    "            ax1.set_xlabel('Episode')\n",
    "            ax1.set_ylabel('Reward', color='tab:blue')\n",
    "            ax1.plot(episodes_x, training_stats['episode_rewards'][-recent_episodes:], color='tab:blue')\n",
    "            ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Create second y-axis for action std\n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.set_ylabel('Action STD', color='tab:red')\n",
    "            ax2.plot([agent.action_std] * recent_episodes, color='tab:red', linestyle='--')\n",
    "            ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "            plt.title('Reward vs. Exploration (Action STD)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot_filename, dpi=300)  # Higher DPI for better quality\n",
    "        \n",
    "        # Also save a copy as the \"latest\" plot for easy access\n",
    "        plt.savefig(os.path.join(plots_dir, f\"latest_metrics.png\"), dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (episode + 1) % checkpoint_interval == 0 or episode == max_episodes - 1:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"agent_checkpoint_ep{episode+1}\")\n",
    "        agent.save(checkpoint_path)\n",
    "        print(f\"Checkpoint saved at episode {episode+1}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    curr_avg_reward = agent.get_average_reward()\n",
    "    if episode > min_episodes_before_early_stop:\n",
    "        if curr_avg_reward > best_avg_reward:\n",
    "            best_avg_reward = curr_avg_reward\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            best_model_path = os.path.join(checkpoint_dir, \"agent_best\")\n",
    "            agent.save(best_model_path)\n",
    "            print(f\"New best model saved with avg reward: {best_avg_reward:.2f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        # Check if we should stop early - UNCOMMENTED TO AVOID EARLY STOPPING\n",
    "        # if patience_counter >= early_stop_patience:\n",
    "        #     print(f\"Early stopping triggered after {episode+1} episodes\")\n",
    "        #     print(f\"Loading best model with avg reward: {best_avg_reward:.2f}\")\n",
    "        #     agent.load(best_model_path)\n",
    "        #     break\n",
    "    \n",
    "    # Periodically run evaluation with no exploration\n",
    "    if (episode + 1) % evaluation_interval == 0:\n",
    "        print(f\"Running evaluation at episode {episode+1}...\")\n",
    "        eval_rewards = []\n",
    "        \n",
    "        # Run multiple evaluation episodes\n",
    "        for eval_ep in range(5):\n",
    "            eval_obs, _ = env.reset()\n",
    "            eval_reward = 0\n",
    "            agent.reset()\n",
    "            \n",
    "            for _ in range(max_timesteps):\n",
    "                eval_action = agent.sample_action(eval_obs, deterministic=True)\n",
    "                eval_obs, eval_r, eval_term, eval_trunc, _ = env.step(eval_action)\n",
    "                eval_reward += eval_r\n",
    "                \n",
    "                if eval_term or eval_trunc:\n",
    "                    break\n",
    "                    \n",
    "            eval_rewards.append(eval_reward)\n",
    "        \n",
    "        avg_eval_reward = np.mean(eval_rewards)\n",
    "        print(f\"Evaluation complete - Avg reward over 5 episodes: {avg_eval_reward:.2f}\")\n",
    "\n",
    "# End of training\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Training completed in {total_time/60:.2f} minutes\")\n",
    "print(f\"Best average reward: {best_avg_reward:.2f}\")\n",
    "\n",
    "# Final evaluation\n",
    "print(\"Running final evaluation...\")\n",
    "final_eval_rewards = []\n",
    "env.video = True  # Record final evaluation\n",
    "\n",
    "for eval_ep in range(10):\n",
    "    eval_obs, _ = env.reset()\n",
    "    eval_reward = 0\n",
    "    agent.reset()\n",
    "    \n",
    "    for _ in range(max_timesteps):\n",
    "        eval_action = agent.sample_action(eval_obs, deterministic=True)\n",
    "        eval_obs, eval_r, eval_term, eval_trunc, _ = env.step(eval_action)\n",
    "        eval_reward += eval_r\n",
    "        \n",
    "        if eval_term or eval_trunc:\n",
    "            break\n",
    "            \n",
    "    final_eval_rewards.append(eval_reward)\n",
    "\n",
    "final_avg_reward = np.mean(final_eval_rewards)\n",
    "print(f\"Final evaluation - Avg reward over 10 episodes: {final_avg_reward:.2f}\")\n",
    "\n",
    "# Write log file\n",
    "env.write_log(folder=metrics_dir, file=\"agent-training-log.txt\")\n",
    "\n",
    "# Save final plot of complete training history\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(training_stats['avg_rewards'])\n",
    "plt.title('Average Reward')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward (last 100 episodes)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(training_stats['policy_losses'], label='Policy Loss')\n",
    "plt.plot(training_stats['value_losses'], label='Value Loss')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(training_stats['episode_lengths'])\n",
    "plt.title('Episode Lengths')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Steps')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(training_stats['learning_rates'])\n",
    "plt.title('Learning Rate')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, \"complete_training_history.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Save final metrics summary as JSON\n",
    "summary = {\n",
    "    'total_episodes': episode + 1,\n",
    "    'max_episodes': max_episodes,\n",
    "    'early_stopped': patience_counter >= early_stop_patience,\n",
    "    'training_duration_seconds': time.time() - start_time,\n",
    "    'final_avg_reward': final_avg_reward,\n",
    "    'best_avg_reward': best_avg_reward,\n",
    "    'final_learning_rate': agent.learning_rate,\n",
    "    'final_action_std': agent.action_std,\n",
    "    'best_model_path': os.path.join(checkpoint_dir, \"agent_best.pt\"),\n",
    "    'final_model_path': os.path.join(checkpoint_dir, f\"agent_checkpoint_ep{episode+1}.pt\"),\n",
    "    'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "\n",
    "with open(os.path.join(metrics_dir, \"training_summary.json\"), 'w') as f:\n",
    "    json.dump(summary, f, indent=4)\n",
    "\n",
    "# Generate a training report in markdown\n",
    "with open(os.path.join(metrics_dir, \"training_report.md\"), 'w') as f:\n",
    "    f.write(f\"# Bipedal Walker Training Report\\n\\n\")\n",
    "    f.write(f\"Generated on: {summary['timestamp']}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"## Training Summary\\n\\n\")\n",
    "    f.write(f\"- Total episodes: {summary['total_episodes']} / {summary['max_episodes']}\\n\")\n",
    "    f.write(f\"- Early stopping: {'Yes' if summary['early_stopped'] else 'No'}\\n\")\n",
    "    f.write(f\"- Training duration: {summary['training_duration_seconds']/60:.2f} minutes\\n\")\n",
    "    f.write(f\"- Final average reward: {summary['final_avg_reward']:.2f}\\n\")\n",
    "    f.write(f\"- Best average reward: {summary['best_avg_reward']:.2f}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"## Hyperparameters\\n\\n\")\n",
    "    f.write(f\"- Initial learning rate: {initial_lr}\\n\")\n",
    "    f.write(f\"- Final learning rate: {summary['final_learning_rate']}\\n\")\n",
    "    f.write(f\"- Initial action STD: {agent.init_action_std}\\n\")\n",
    "    f.write(f\"- Final action STD: {summary['final_action_std']}\\n\")\n",
    "    f.write(f\"- Gamma (discount factor): {agent.gamma}\\n\")\n",
    "    f.write(f\"- GAE lambda: {agent.gae_lambda}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"## Model Files\\n\\n\")\n",
    "    f.write(f\"- Best model: `{summary['best_model_path']}`\\n\")\n",
    "    f.write(f\"- Final model: `{summary['final_model_path']}`\\n\\n\")\n",
    "    \n",
    "    f.write(f\"## Performance Analysis\\n\\n\")\n",
    "    f.write(f\"![Complete Training History](plots/complete_training_history.png)\\n\\n\")\n",
    "    f.write(f\"![Latest Metrics](plots/latest_metrics.png)\\n\\n\")\n",
    "\n",
    "print(f\"Complete metrics saved to {metrics_dir} directory\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small demo with a predefined heuristic that is suboptimal and has no notion of balance (and is designed for the orignal BipedalWalker environment)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.box2d.bipedal_walker import BipedalWalkerHeuristics\n",
    "\n",
    "env = rld.make(\n",
    "    \"rldurham/Walker\",\n",
    "    # \"BipedalWalker-v3\",\n",
    "    render_mode=\"human\",\n",
    "    # render_mode=\"rgb_array\",\n",
    "    hardcore=False,\n",
    "    # hardcore=True,\n",
    ")\n",
    "_, obs, info = rld.seed_everything(42, env)\n",
    "\n",
    "heuristics = BipedalWalkerHeuristics()\n",
    "\n",
    "act = heuristics.step_heuristic(obs)\n",
    "for _ in range(500):\n",
    "    obs, rew, terminated, truncated, info = env.step(act)\n",
    "    act = heuristics.step_heuristic(obs)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    if env.render_mode == \"rgb_array\":\n",
    "        rld.render(env, clear=True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
