{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NKFN77 - William Stapleton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTNU1mwGB1ZD"
   },
   "source": [
    "**Dependencies and imports**\n",
    "\n",
    "This can take a minute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install swig\n",
    "# !pip install --upgrade rldurham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import rldurham as rld\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJHtclV_30Re"
   },
   "source": [
    "## Reinforcement Learning set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4jXNHP8_U-rn"
   },
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(Agent, self).__init__()\n",
    "        \n",
    "        # Initialize networks with orthogonal weights for better gradient flow\n",
    "        self.policy = self._build_policy_network(state_dim, action_dim, hidden_dim)\n",
    "        self.value = self._build_value_network(state_dim, hidden_dim)\n",
    "        \n",
    "        # Hyperparameters tuned for stability\n",
    "        self.gamma = 0.99\n",
    "        self.learning_rate = 3e-4 #4e-4 was good\n",
    "        self.gae_lambda = 0.95\n",
    "        self.clip_ratio = 0.2\n",
    "        self.entropy_coef = 0.01  # Encourage exploration\n",
    "        self.value_coef = 0.5    # Balance value and policy learning\n",
    "        \n",
    "        # Adaptive noise control\n",
    "        self.init_action_std = 0.6\n",
    "        self.action_std = self.init_action_std\n",
    "        self.action_std_decay = 0.999\n",
    "        self.min_action_std = 0.2\n",
    "        self.noise_decay_start = 1000  # Start decay after 100 episodes # 500 was good\n",
    "        \n",
    "        # Experience management\n",
    "        self.trajectory = []\n",
    "        self.experience_buffer = []  # Store successful episodes\n",
    "        self.buffer_size = 10000     # Maximum buffer size\n",
    "        self.success_threshold = 50  # Threshold to consider an episode \"successful\"\n",
    "        self.replay_ratio = 0.3  \n",
    "        \n",
    "        self.value_normalizer = RunningMeanStd()\n",
    "        self.state_normalizer = RunningMeanStd(shape=state_dim)\n",
    "        self.reward_normalizer = RunningMeanStd()\n",
    "        \n",
    "        # Optimization\n",
    "        self.policy_optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        self.value_optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        self.snapshot_interval = 100  # Save model snapshot every N episodes\n",
    "        self.snapshots = []\n",
    "        self.max_snapshots = 3\n",
    "        self.snapshot_weights = [0.7, 0.2, 0.1]  # Weights for ensemble predictions\n",
    "        \n",
    "        # Statistics tracking\n",
    "        self.running_rewards = deque(maxlen=100)\n",
    "        self.early_stopping_patience = 50\n",
    "        self.patience_counter = 0\n",
    "        self.episode_count = 0\n",
    "        self.best_reward = float('-inf')\n",
    "        self.best_avg_reward = float('-inf')\n",
    "        self.recent_actions = deque(maxlen=5)  # For action smoothing\n",
    "        \n",
    "    def _build_policy_network(self, state_dim, action_dim, hidden_dim):\n",
    "        \"\"\"Build policy network with proper initialization\"\"\"\n",
    "        policy = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Orthogonal initialization for better training dynamics\n",
    "        for layer in policy:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                torch.nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))\n",
    "                torch.nn.init.zeros_(layer.bias)\n",
    "        \n",
    "        return policy\n",
    "    \n",
    "    def _build_value_network(self, state_dim, hidden_dim):\n",
    "        \"\"\"Build value network with proper initialization\"\"\"\n",
    "        value = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        for layer in value:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                torch.nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))\n",
    "                torch.nn.init.zeros_(layer.bias)\n",
    "        \n",
    "        return value\n",
    "    \n",
    "    def normalize_state(self, state):\n",
    "        \"\"\"Normalize state using running statistics\"\"\"\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.FloatTensor(state)\n",
    "        return torch.FloatTensor(self.state_normalizer(state.numpy()))\n",
    "    \n",
    "    def normalize_reward(self, reward):\n",
    "        \"\"\"Normalize rewards for more stable learning\"\"\"\n",
    "        self.reward_normalizer(np.array([reward]))\n",
    "        return reward\n",
    "    \n",
    "    def sample_action(self, state, deterministic=False):\n",
    "        \"\"\"Sample action with adaptive noise and temporal smoothing\"\"\"\n",
    "        state = self.normalize_state(state)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            mean_action = self.policy(state)\n",
    "            \n",
    "            if deterministic:\n",
    "                action = mean_action\n",
    "            else:\n",
    "                # Add exploration noise with adaptive standard deviation\n",
    "                noise = torch.randn_like(mean_action) * self.action_std\n",
    "                action = torch.clamp(mean_action + noise, -1, 1)\n",
    "                \n",
    "                # Apply temporal smoothing for more natural movements\n",
    "                if len(self.recent_actions) > 0:\n",
    "                    smooth_factor = 0.7\n",
    "                    prev_action = np.mean([a for a in self.recent_actions], axis=0)\n",
    "                    action = smooth_factor * action + (1 - smooth_factor) * torch.FloatTensor(prev_action)\n",
    "                \n",
    "                # Ensemble prediction from snapshots for more robustness\n",
    "                if len(self.snapshots) > 0 and np.random.random() < 0.3:  # 30% chance to use ensemble\n",
    "                    ensemble_actions = [snapshot_policy(state) for snapshot_policy, _ in self.snapshots]\n",
    "                    ensemble_actions.append(action)  # Include current policy\n",
    "                    \n",
    "                    # Weight the actions based on recency\n",
    "                    weights = self.snapshot_weights[:len(self.snapshots)] + [1.0]\n",
    "                    weights = [w/sum(weights) for w in weights]\n",
    "                    \n",
    "                    action = sum(w * a for w, a in zip(weights, ensemble_actions))\n",
    "                    action = torch.clamp(action, -1, 1)\n",
    "            \n",
    "            self.recent_actions.append(action.numpy())\n",
    "        \n",
    "        return action.numpy()\n",
    "    \n",
    "    def put_data(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store transition with normalized states\"\"\"\n",
    "        state = self.normalize_state(state)\n",
    "        next_state = self.normalize_state(next_state)\n",
    "        norm_reward = self.normalize_reward(reward)\n",
    "        self.trajectory.append((state, action, norm_reward, next_state, done))\n",
    "    \n",
    "    def add_to_experience_buffer(self, episode_data, episode_reward):\n",
    "        \"\"\"Add successful episodes to the experience buffer\"\"\"\n",
    "        if episode_reward > self.success_threshold:\n",
    "            self.experience_buffer.extend(episode_data)\n",
    "            # Trim buffer if it gets too large\n",
    "            if len(self.experience_buffer) > self.buffer_size:\n",
    "                excess = len(self.experience_buffer) - self.buffer_size\n",
    "                self.experience_buffer = self.experience_buffer[excess:]\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Update policy and value networks with improved stability measures\"\"\"\n",
    "        if len(self.trajectory) < 1:\n",
    "            return 0, 0\n",
    "        \n",
    "        # Check if we should add the trajectory to the experience buffer\n",
    "        episode_reward = sum([r for _, _, r, _, _ in self.trajectory])\n",
    "        self.add_to_experience_buffer(self.trajectory, episode_reward)\n",
    "        \n",
    "        # Mix in some past successful experiences if available\n",
    "        if len(self.experience_buffer) > 0 and np.random.random() < self.replay_ratio:\n",
    "            replay_size = min(len(self.experience_buffer), int(len(self.trajectory) * 0.5))\n",
    "            replay_samples = random.sample(self.experience_buffer, replay_size)\n",
    "            training_data = self.trajectory + replay_samples\n",
    "        else:\n",
    "            training_data = self.trajectory\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = zip(*training_data)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.stack(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "        \n",
    "        # Compute normalized returns and advantages\n",
    "        with torch.no_grad():\n",
    "            values = self.value(states).squeeze()\n",
    "            next_values = self.value(next_states).squeeze()\n",
    "            \n",
    "            # Compute GAE\n",
    "            advantages = torch.zeros_like(rewards)\n",
    "            gae = 0\n",
    "            for t in reversed(range(len(rewards))):\n",
    "                if t == len(rewards) - 1:\n",
    "                    next_value = next_values[t]\n",
    "                else:\n",
    "                    next_value = values[t + 1]\n",
    "                \n",
    "                delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]\n",
    "                gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae\n",
    "                advantages[t] = gae\n",
    "            \n",
    "            returns = advantages + values\n",
    "            \n",
    "            # Normalize advantages\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Compute action probabilities\n",
    "        mean_actions = self.policy(states)\n",
    "        dist = torch.distributions.Normal(mean_actions, self.action_std)\n",
    "        old_log_probs = dist.log_prob(actions).sum(dim=1)\n",
    "        \n",
    "        # Multiple epochs of optimization with early stopping\n",
    "        policy_losses = []\n",
    "        value_losses = []\n",
    "        kl_divs = []\n",
    "        \n",
    "        for epoch in range(10):  # 10 epochs max\n",
    "            # Compute new action probabilities\n",
    "            mean_actions = self.policy(states)\n",
    "            dist = torch.distributions.Normal(mean_actions, self.action_std)\n",
    "            new_log_probs = dist.log_prob(actions).sum(dim=1)\n",
    "            \n",
    "            # Compute entropy for exploration\n",
    "            entropy = dist.entropy().mean()\n",
    "            \n",
    "            # Compute policy loss with clipping\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs.detach())\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # Compute value loss with clipping\n",
    "            value_pred = self.value(states).squeeze()\n",
    "            value_clipped = values + torch.clamp(value_pred - values, -self.clip_ratio, self.clip_ratio)\n",
    "            value_loss_1 = F.mse_loss(value_pred, returns.detach())\n",
    "            value_loss_2 = F.mse_loss(value_clipped, returns.detach())\n",
    "            value_loss = torch.max(value_loss_1, value_loss_2)\n",
    "            \n",
    "            # Compute KL divergence for early stopping\n",
    "            approx_kl = ((old_log_probs - new_log_probs) * ratio).mean().item()\n",
    "            kl_divs.append(approx_kl)\n",
    "            \n",
    "            # Store losses\n",
    "            policy_losses.append(policy_loss.item())\n",
    "            value_losses.append(value_loss.item())\n",
    "            \n",
    "            # Early stopping based on KL divergence\n",
    "            if approx_kl > 0.015:\n",
    "                break\n",
    "                \n",
    "            # Update policy network\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss_with_entropy = policy_loss - self.entropy_coef * entropy\n",
    "            policy_loss_with_entropy.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), max_norm=0.5)\n",
    "            self.policy_optimizer.step()\n",
    "            \n",
    "            # Update value network separately for more stability\n",
    "            self.value_optimizer.zero_grad()\n",
    "            (self.value_coef * value_loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.value.parameters(), max_norm=0.5)\n",
    "            self.value_optimizer.step()\n",
    "        \n",
    "        # Clear trajectory buffer\n",
    "        self.trajectory = []\n",
    "        \n",
    "        # Update exploration noise\n",
    "        if self.episode_count > self.noise_decay_start:\n",
    "            self.action_std = max(self.min_action_std, \n",
    "                                self.action_std * self.action_std_decay)\n",
    "        \n",
    "        return np.mean(policy_losses), np.mean(value_losses)\n",
    "    \n",
    "    def update_stats(self, episode_reward):\n",
    "        \"\"\"Update running statistics and episode count\"\"\"\n",
    "        self.running_rewards.append(episode_reward)\n",
    "        self.episode_count += 1\n",
    "        self.best_reward = max(self.best_reward, episode_reward)\n",
    "        \n",
    "        # Early stopping check\n",
    "        current_avg_reward = self.get_average_reward()\n",
    "        if current_avg_reward > self.best_avg_reward:\n",
    "            self.best_avg_reward = current_avg_reward\n",
    "            self.patience_counter = 0\n",
    "            \n",
    "            # Save model snapshot for ensemble\n",
    "            if self.episode_count % self.snapshot_interval == 0:\n",
    "                policy_copy = copy.deepcopy(self.policy)\n",
    "                value_copy = copy.deepcopy(self.value)\n",
    "                self.snapshots.append((policy_copy, value_copy))\n",
    "                if len(self.snapshots) > self.max_snapshots:\n",
    "                    self.snapshots.pop(0)  # Remove oldest snapshot\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "            \n",
    "            # If patience exceeded, revert to best snapshot\n",
    "            if self.patience_counter >= self.early_stopping_patience and len(self.snapshots) > 0:\n",
    "                print(f\"Performance plateaued for {self.early_stopping_patience} episodes. Reverting to previous best model.\")\n",
    "                latest_snapshot = self.snapshots[-1]\n",
    "                self.policy = copy.deepcopy(latest_snapshot[0])\n",
    "                self.value = copy.deepcopy(latest_snapshot[1])\n",
    "                self.patience_counter = 0\n",
    "    \n",
    "    def get_average_reward(self):\n",
    "        \"\"\"Calculate average reward over last 100 episodes\"\"\"\n",
    "        return np.mean(self.running_rewards) if self.running_rewards else 0\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset temporal smoothing between episodes\"\"\"\n",
    "        self.recent_actions.clear()\n",
    "        \n",
    "    def save(self, filename):\n",
    "        \"\"\"\n",
    "        Save model state with enhanced security and compatibility.\n",
    "        This version handles separate optimizers for policy and value networks.\n",
    "        \"\"\"\n",
    "        # Save neural network states and their optimizers separately\n",
    "        network_state = {\n",
    "            'policy_state_dict': self.policy.state_dict(),\n",
    "            'value_state_dict': self.value.state_dict(),\n",
    "            'policy_optimizer_state_dict': self.policy_optimizer.state_dict(),\n",
    "            'value_optimizer_state_dict': self.value_optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(network_state, filename + '_networks.pt')\n",
    "        \n",
    "        # Save normalizer statistics as plain numbers\n",
    "        normalizer_state = {\n",
    "            'state_mean': self.state_normalizer.mean.tolist(),\n",
    "            'state_var': self.state_normalizer.var.tolist(),\n",
    "            'state_count': float(self.state_normalizer.count),\n",
    "            'value_mean': self.value_normalizer.mean.tolist(),\n",
    "            'value_var': self.value_normalizer.var.tolist(),\n",
    "            'value_count': float(self.value_normalizer.count),\n",
    "        }\n",
    "        \n",
    "        # Save other parameters\n",
    "        other_state = {\n",
    "            'episode_count': self.episode_count,\n",
    "            'best_reward': float(self.best_reward),\n",
    "            'action_std': float(self.action_std),\n",
    "            'normalizer_state': normalizer_state\n",
    "        }\n",
    "        \n",
    "        # Save as JSON for better compatibility\n",
    "        with open(filename + '_other.json', 'w') as f:\n",
    "            json.dump(other_state, f)\n",
    "\n",
    "    def load(self, filename):\n",
    "        \"\"\"\n",
    "        Load model state with enhanced security and compatibility.\n",
    "        This version handles separate optimizers for policy and value networks.\n",
    "        \"\"\"\n",
    "        # Load neural network states\n",
    "        network_state = torch.load(filename + '_networks.pt', weights_only=True)\n",
    "        self.policy.load_state_dict(network_state['policy_state_dict'])\n",
    "        self.value.load_state_dict(network_state['value_state_dict'])\n",
    "        self.policy_optimizer.load_state_dict(network_state['policy_optimizer_state_dict'])\n",
    "        self.value_optimizer.load_state_dict(network_state['value_optimizer_state_dict'])\n",
    "        \n",
    "        # Load other parameters from JSON\n",
    "        try:\n",
    "            with open(filename + '_other.json', 'r') as f:\n",
    "                other_state = json.load(f)\n",
    "                \n",
    "            # Restore normalizer states\n",
    "            normalizer_state = other_state['normalizer_state']\n",
    "            \n",
    "            # Reconstruct state normalizer\n",
    "            self.state_normalizer.mean = np.array(normalizer_state['state_mean'], dtype=np.float32)\n",
    "            self.state_normalizer.var = np.array(normalizer_state['state_var'], dtype=np.float32)\n",
    "            self.state_normalizer.count = normalizer_state['state_count']\n",
    "            \n",
    "            # Reconstruct value normalizer\n",
    "            self.value_normalizer.mean = np.array(normalizer_state['value_mean'], dtype=np.float32)\n",
    "            self.value_normalizer.var = np.array(normalizer_state['value_var'], dtype=np.float32)\n",
    "            self.value_normalizer.count = normalizer_state['value_count']\n",
    "            \n",
    "            # Restore other parameters\n",
    "            self.episode_count = other_state['episode_count']\n",
    "            self.best_reward = other_state['best_reward']\n",
    "            self.action_std = other_state['action_std']\n",
    "            \n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "            print(f\"Warning: Could not load complete state. Only network weights were restored. Error: {e}\")\n",
    "\n",
    "class RunningMeanStd:\n",
    "    \"\"\"Tracks running mean and standard deviation for normalization\"\"\"\n",
    "    def __init__(self, shape=(), epsilon=1e-4):\n",
    "        self.mean = np.zeros(shape, dtype=np.float32)\n",
    "        self.var = np.ones(shape, dtype=np.float32)\n",
    "        self.count = epsilon\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_var = np.var(x, axis=0)\n",
    "        batch_count = x.shape[0] if len(x.shape) > 1 else 1\n",
    "        \n",
    "        delta = batch_mean - self.mean\n",
    "        self.mean += delta * batch_count / (self.count + batch_count)\n",
    "        m_a = self.var * self.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / (self.count + batch_count)\n",
    "        self.var = M2 / (self.count + batch_count)\n",
    "        self.count += batch_count\n",
    "        \n",
    "        return (x - self.mean) / np.sqrt(self.var + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEv4ZjXmyrHo"
   },
   "source": [
    "### Prepare the environment and wrap it to capture statistics, logs, and video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1Xrcek4hxDXl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is: cpu (as recommended)\n",
      "actions are continuous with 4 dimensions/#actions\n",
      "observations are continuous with 24 dimensions/#observations\n",
      "maximum timesteps is: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/billy/.local/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/billy/Github/ReinforcementLearning/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\")\n",
    "env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\", hardcore=True) # only attempt this when your agent has solved the non-hardcore version\n",
    "\n",
    "# get statistics, logs, and videos\n",
    "env = rld.Recorder(\n",
    "    env,\n",
    "    smoothing=10,                       # track rolling averages (useful for plotting)\n",
    "    video=True,                         # enable recording videos\n",
    "    video_folder=\"videos\",              # folder for videos\n",
    "    video_prefix=\"nkfn77-agent-video\",  # prefix for videos (replace xxxx00 with your username)\n",
    "    logs=True,                          # keep logs\n",
    ")\n",
    "\n",
    "# training on CPU recommended\n",
    "rld.check_device()\n",
    "\n",
    "# environment info\n",
    "discrete_act, discrete_obs, act_dim, obs_dim = rld.env_info(env, print_out=True)\n",
    "\n",
    "# render start image\n",
    "env.reset(seed=42)\n",
    "rld.render(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment doesn't support custom video paths. Videos may be saved elsewhere.\n",
      "Environment vectorization not available, using standard environment\n",
      "Episode 1/1000, Reward: -99.28, Avg Reward: -99.28, Action STD: 0.600, Episode Length: 69, Time: 0.3s, Est. Remaining: 4.8min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 123\u001b[0m\n\u001b[1;32m    120\u001b[0m         agent\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m new_lr\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# Train agent in batches (more efficient)\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     policy_loss, value_loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     policy_loss, value_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# No training during evaluation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 254\u001b[0m, in \u001b[0;36mAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m policy_loss_with_entropy\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    253\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m--> 254\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# Update value network separately for more stability\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:244\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    235\u001b[0m         group,\n\u001b[1;32m    236\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m         state_steps,\n\u001b[1;32m    242\u001b[0m     )\n\u001b[0;32m--> 244\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:876\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    874\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 876\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:478\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    476\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 478\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# in the submission please use seed_everything with seed 42 for verification\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Ensure reproducibility across runs\n",
    "seed, observation, info = rld.seed_everything(42, env)\n",
    "\n",
    "# Initialize agent\n",
    "agent = Agent(env.observation_space.shape[0], env.action_space.shape[0])\n",
    "max_episodes = 1000\n",
    "max_timesteps = 2000\n",
    "\n",
    "# Track statistics for plotting with enhanced metrics\n",
    "tracker = rld.InfoTracker()\n",
    "evaluation_interval = 50  # Evaluate agent without exploration every N episodes\n",
    "checkpoint_interval = 500  # Save model checkpoint every N episodes\n",
    "early_stop_patience = 200  # Episodes to wait before early stopping\n",
    "best_avg_reward = float('-inf')\n",
    "patience_counter = 0\n",
    "min_episodes_before_early_stop = 1000  # Minimum training episodes before considering early stopping\n",
    "\n",
    "# Create directories for checkpoints and metrics\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "metrics_dir = \"metrics\"\n",
    "plots_dir = os.path.join(metrics_dir, \"plots\")\n",
    "csv_dir = os.path.join(metrics_dir, \"csv\")\n",
    "video_dir = os.path.join(metrics_dir, \"videos\")\n",
    "\n",
    "# Create all necessary directories\n",
    "for directory in [checkpoint_dir, metrics_dir, plots_dir, csv_dir, video_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Configure video path if the environment supports it\n",
    "try:\n",
    "    env.set_video_path(video_dir)\n",
    "except:\n",
    "    print(f\"Environment doesn't support custom video paths. Videos may be saved elsewhere.\")\n",
    "\n",
    "# Initialize training metrics tracking\n",
    "training_stats = {\n",
    "    'episode_rewards': [],\n",
    "    'avg_rewards': [],\n",
    "    'policy_losses': [],\n",
    "    'value_losses': [],\n",
    "    'episode_lengths': [],\n",
    "    'learning_rates': []\n",
    "}\n",
    "\n",
    "# Vectorize environment operations if possible\n",
    "# Note: This is a placeholder - actual implementation depends on the env type\n",
    "try:\n",
    "    env = rld.make_env_faster(env)  # Fictional function - replace with actual vectorization if available\n",
    "except:\n",
    "    print(\"Environment vectorization not available, using standard environment\")\n",
    "\n",
    "# Learning rate scheduler for gradual LR reduction\n",
    "initial_lr = agent.learning_rate\n",
    "min_lr = initial_lr / 10\n",
    "lr_decay_factor = 0.995\n",
    "lr_decay_start = 1000\n",
    "\n",
    "# Start training timer\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    # Set up episode - only record video occasionally to save time\n",
    "    is_evaluation_episode = episode % evaluation_interval == 0\n",
    "    env.info = episode % 10 == 0\n",
    "    env.video = episode % 250 == 0  # Reduced frequency for video recording\n",
    "    \n",
    "    current_observation, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_start_time = time.time()\n",
    "    \n",
    "    # Reset agent's temporal smoothing\n",
    "    agent.reset()\n",
    "    \n",
    "    # Store transitions for batch processing\n",
    "    episode_transitions = []\n",
    "    \n",
    "    # Run episode\n",
    "    for t in range(max_timesteps):\n",
    "        # Get action from agent (deterministic for evaluation episodes)\n",
    "        action = agent.sample_action(current_observation, deterministic=is_evaluation_episode)\n",
    "        \n",
    "        # Take step in environment\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Store transition\n",
    "        if not is_evaluation_episode:  # Don't store evaluation episodes\n",
    "            agent.put_data(current_observation, action, reward, next_observation, \n",
    "                          terminated or truncated)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        episode_transitions.append((current_observation, action, reward, next_observation, \n",
    "                                   terminated or truncated))\n",
    "        \n",
    "        # Update observation\n",
    "        current_observation = next_observation\n",
    "        \n",
    "        # Check if episode is done\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    # Only update the agent if this wasn't an evaluation episode\n",
    "    if not is_evaluation_episode:\n",
    "        # Update learning rate according to schedule\n",
    "        if episode > lr_decay_start:\n",
    "            new_lr = max(min_lr, initial_lr * (lr_decay_factor ** (episode - lr_decay_start)))\n",
    "            for param_group in agent.policy_optimizer.param_groups:\n",
    "                param_group['lr'] = new_lr\n",
    "            for param_group in agent.value_optimizer.param_groups:\n",
    "                param_group['lr'] = new_lr\n",
    "            agent.learning_rate = new_lr\n",
    "            \n",
    "        # Train agent in batches (more efficient)\n",
    "        policy_loss, value_loss = agent.train()\n",
    "    else:\n",
    "        policy_loss, value_loss = 0, 0  # No training during evaluation\n",
    "    \n",
    "    # Update agent statistics\n",
    "    agent.update_stats(episode_reward)\n",
    "    \n",
    "    # Save metrics\n",
    "    episode_length = t + 1\n",
    "    training_stats['episode_rewards'].append(episode_reward)\n",
    "    training_stats['avg_rewards'].append(agent.get_average_reward())\n",
    "    training_stats['policy_losses'].append(policy_loss)\n",
    "    training_stats['value_losses'].append(value_loss)\n",
    "    training_stats['episode_lengths'].append(episode_length)\n",
    "    training_stats['learning_rates'].append(agent.learning_rate)\n",
    "    \n",
    "    # Save metrics to CSV after each episode\n",
    "    if episode == 0:\n",
    "        # Create CSV files with headers\n",
    "        metrics_files = {\n",
    "            'rewards': os.path.join(csv_dir, 'episode_rewards.csv'),\n",
    "            'avg_rewards': os.path.join(csv_dir, 'avg_rewards.csv'),\n",
    "            'losses': os.path.join(csv_dir, 'losses.csv'),\n",
    "            'episode_data': os.path.join(csv_dir, 'episode_data.csv')\n",
    "        }\n",
    "        \n",
    "        # Initialize CSV files with headers\n",
    "        with open(metrics_files['rewards'], 'w') as f:\n",
    "            f.write('episode,reward\\n')\n",
    "        \n",
    "        with open(metrics_files['avg_rewards'], 'w') as f:\n",
    "            f.write('episode,avg_reward_last_100\\n')\n",
    "            \n",
    "        with open(metrics_files['losses'], 'w') as f:\n",
    "            f.write('episode,policy_loss,value_loss\\n')\n",
    "            \n",
    "        with open(metrics_files['episode_data'], 'w') as f:\n",
    "            f.write('episode,length,learning_rate,action_std\\n')\n",
    "    \n",
    "    # Append data to CSV files\n",
    "    with open(os.path.join(csv_dir, 'episode_rewards.csv'), 'a') as f:\n",
    "        f.write(f\"{episode},{episode_reward}\\n\")\n",
    "        \n",
    "    with open(os.path.join(csv_dir, 'avg_rewards.csv'), 'a') as f:\n",
    "        f.write(f\"{episode},{agent.get_average_reward()}\\n\")\n",
    "        \n",
    "    with open(os.path.join(csv_dir, 'losses.csv'), 'a') as f:\n",
    "        f.write(f\"{episode},{policy_loss},{value_loss}\\n\")\n",
    "        \n",
    "    with open(os.path.join(csv_dir, 'episode_data.csv'), 'a') as f:\n",
    "        f.write(f\"{episode},{episode_length},{agent.learning_rate},{agent.action_std}\\n\")\n",
    "    \n",
    "    # Print progress with more detailed information\n",
    "    if episode % 10 == 0:\n",
    "        avg_reward = agent.get_average_reward()\n",
    "        elapsed_time = time.time() - start_time\n",
    "        eps_per_second = (episode + 1) / elapsed_time if elapsed_time > 0 else 0\n",
    "        estimated_remaining = (max_episodes - episode - 1) / eps_per_second / 60 if eps_per_second > 0 else 0\n",
    "        \n",
    "        print(f\"Episode {episode+1}/{max_episodes}, Reward: {episode_reward:.2f}, \"\n",
    "              f\"Avg Reward: {avg_reward:.2f}, Action STD: {agent.action_std:.3f}, \"\n",
    "              f\"Episode Length: {episode_length}, Time: {elapsed_time:.1f}s, \"\n",
    "              f\"Est. Remaining: {estimated_remaining:.1f}min\")\n",
    "    \n",
    "    # Track and plot\n",
    "    tracker.track(info)\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        tracker.plot(r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "        \n",
    "        # Add additional plots for training metrics\n",
    "        # Create plot filename with timestamp for easier organization\n",
    "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        plot_filename = os.path.join(plots_dir, f\"metrics_ep{episode+1}_{timestamp}.png\")\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.plot(training_stats['episode_rewards'][-100:])\n",
    "        plt.title('Recent Episode Rewards')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.plot(training_stats['policy_losses'][-100:], label='Policy Loss')\n",
    "        plt.plot(training_stats['value_losses'][-100:], label='Value Loss')\n",
    "        plt.title('Losses')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 3, 3)\n",
    "        plt.plot(training_stats['episode_lengths'][-100:])\n",
    "        plt.title('Episode Lengths')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Steps')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 3, 4)\n",
    "        plt.plot(training_stats['learning_rates'])\n",
    "        plt.title('Learning Rate')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('LR')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 3, 5)\n",
    "        # Moving average of rewards for smoothed trend\n",
    "        window_size = min(25, len(training_stats['episode_rewards']))\n",
    "        if window_size > 1:\n",
    "            smoothed_rewards = np.convolve(\n",
    "                training_stats['episode_rewards'], \n",
    "                np.ones(window_size)/window_size, \n",
    "                mode='valid'\n",
    "            )\n",
    "            plt.plot(smoothed_rewards)\n",
    "            plt.title(f'Smoothed Rewards (window={window_size})')\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Smoothed Reward')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 3, 6)\n",
    "        # Compare action standard deviation with episode rewards\n",
    "        recent_episodes = min(100, len(training_stats['episode_rewards']))\n",
    "        if recent_episodes > 0:\n",
    "            episodes_x = list(range(len(training_stats['episode_rewards'])))[-recent_episodes:]\n",
    "            ax1 = plt.gca()\n",
    "            ax1.set_xlabel('Episode')\n",
    "            ax1.set_ylabel('Reward', color='tab:blue')\n",
    "            ax1.plot(episodes_x, training_stats['episode_rewards'][-recent_episodes:], color='tab:blue')\n",
    "            ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Create second y-axis for action std\n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.set_ylabel('Action STD', color='tab:red')\n",
    "            ax2.plot([agent.action_std] * recent_episodes, color='tab:red', linestyle='--')\n",
    "            ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "            plt.title('Reward vs. Exploration (Action STD)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot_filename, dpi=300)  # Higher DPI for better quality\n",
    "        \n",
    "        # Also save a copy as the \"latest\" plot for easy access\n",
    "        # plt.savefig(os.path.join(plots_dir, f\"latest_metrics.png\"), dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # Save checkpoint\n",
    "    # if (episode + 1) % checkpoint_interval == 0 or episode == max_episodes - 1:\n",
    "    #     checkpoint_path = os.path.join(checkpoint_dir, f\"agent_checkpoint_ep{episode+1}\")\n",
    "    #     agent.save(checkpoint_path)\n",
    "    #     print(f\"Checkpoint saved at episode {episode+1}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    curr_avg_reward = agent.get_average_reward()\n",
    "    if episode > min_episodes_before_early_stop:\n",
    "        if curr_avg_reward > best_avg_reward:\n",
    "            best_avg_reward = curr_avg_reward\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            best_model_path = os.path.join(checkpoint_dir, \"agent_best\")\n",
    "            agent.save(best_model_path)\n",
    "            print(f\"New best model saved with avg reward: {best_avg_reward:.2f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        # Check if we should stop early - UNCOMMENTED TO AVOID EARLY STOPPING\n",
    "        # if patience_counter >= early_stop_patience:\n",
    "        #     print(f\"Early stopping triggered after {episode+1} episodes\")\n",
    "        #     print(f\"Loading best model with avg reward: {best_avg_reward:.2f}\")\n",
    "        #     agent.load(best_model_path)\n",
    "        #     break\n",
    "    \n",
    "    # Periodically run evaluation with no exploration\n",
    "    if (episode + 1) % evaluation_interval == 0:\n",
    "        print(f\"Running evaluation at episode {episode+1}...\")\n",
    "        eval_rewards = []\n",
    "        \n",
    "        # Run multiple evaluation episodes\n",
    "        for eval_ep in range(5):\n",
    "            eval_obs, _ = env.reset()\n",
    "            eval_reward = 0\n",
    "            agent.reset()\n",
    "            \n",
    "            for _ in range(max_timesteps):\n",
    "                eval_action = agent.sample_action(eval_obs, deterministic=True)\n",
    "                eval_obs, eval_r, eval_term, eval_trunc, _ = env.step(eval_action)\n",
    "                eval_reward += eval_r\n",
    "                \n",
    "                if eval_term or eval_trunc:\n",
    "                    break\n",
    "                    \n",
    "            eval_rewards.append(eval_reward)\n",
    "        \n",
    "        avg_eval_reward = np.mean(eval_rewards)\n",
    "        print(f\"Evaluation complete - Avg reward over 5 episodes: {avg_eval_reward:.2f}\")\n",
    "\n",
    "# End of training\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Training completed in {total_time/60:.2f} minutes\")\n",
    "print(f\"Best average reward: {best_avg_reward:.2f}\")\n",
    "\n",
    "# Final evaluation\n",
    "# print(\"Running final evaluation...\")\n",
    "# final_eval_rewards = []\n",
    "# env.video = True  # Record final evaluation\n",
    "\n",
    "# for eval_ep in range(10):\n",
    "#     eval_obs, _ = env.reset()\n",
    "#     eval_reward = 0\n",
    "#     agent.reset()\n",
    "    \n",
    "#     for _ in range(max_timesteps):\n",
    "#         eval_action = agent.sample_action(eval_obs, deterministic=True)\n",
    "#         eval_obs, eval_r, eval_term, eval_trunc, _ = env.step(eval_action)\n",
    "#         eval_reward += eval_r\n",
    "        \n",
    "#         if eval_term or eval_trunc:\n",
    "#             break\n",
    "            \n",
    "#     final_eval_rewards.append(eval_reward)\n",
    "\n",
    "# final_avg_reward = np.mean(final_eval_rewards)\n",
    "# print(f\"Final evaluation - Avg reward over 10 episodes: {final_avg_reward:.2f}\")\n",
    "\n",
    "# Write log file\n",
    "env.write_log(folder=metrics_dir, file=\"agent-training-log.txt\")\n",
    "\n",
    "# Save final plot of complete training history\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(training_stats['avg_rewards'])\n",
    "plt.title('Average Reward')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward (last 100 episodes)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(training_stats['policy_losses'], label='Policy Loss')\n",
    "plt.plot(training_stats['value_losses'], label='Value Loss')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(training_stats['episode_lengths'])\n",
    "plt.title('Episode Lengths')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Steps')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(training_stats['learning_rates'])\n",
    "plt.title('Learning Rate')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, \"complete_training_history.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Save final metrics summary as JSON\n",
    "# summary = {\n",
    "#     'total_episodes': episode + 1,\n",
    "#     'max_episodes': max_episodes,\n",
    "#     'early_stopped': patience_counter >= early_stop_patience,\n",
    "#     'training_duration_seconds': time.time() - start_time,\n",
    "#     'final_avg_reward': final_avg_reward,\n",
    "#     'best_avg_reward': best_avg_reward,\n",
    "#     'final_learning_rate': agent.learning_rate,\n",
    "#     'final_action_std': agent.action_std,\n",
    "#     'best_model_path': os.path.join(checkpoint_dir, \"agent_best.pt\"),\n",
    "#     'final_model_path': os.path.join(checkpoint_dir, f\"agent_checkpoint_ep{episode+1}.pt\"),\n",
    "#     'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# }\n",
    "\n",
    "# with open(os.path.join(metrics_dir, \"training_summary.json\"), 'w') as f:\n",
    "#     json.dump(summary, f, indent=4)\n",
    "\n",
    "# Generate a training report in markdown\n",
    "# with open(os.path.join(metrics_dir, \"training_report.md\"), 'w') as f:\n",
    "#     f.write(f\"# Bipedal Walker Training Report\\n\\n\")\n",
    "#     f.write(f\"Generated on: {summary['timestamp']}\\n\\n\")\n",
    "    \n",
    "#     f.write(f\"## Training Summary\\n\\n\")\n",
    "#     f.write(f\"- Total episodes: {summary['total_episodes']} / {summary['max_episodes']}\\n\")\n",
    "#     f.write(f\"- Early stopping: {'Yes' if summary['early_stopped'] else 'No'}\\n\")\n",
    "#     f.write(f\"- Training duration: {summary['training_duration_seconds']/60:.2f} minutes\\n\")\n",
    "#     f.write(f\"- Final average reward: {summary['final_avg_reward']:.2f}\\n\")\n",
    "#     f.write(f\"- Best average reward: {summary['best_avg_reward']:.2f}\\n\\n\")\n",
    "    \n",
    "#     f.write(f\"## Hyperparameters\\n\\n\")\n",
    "#     f.write(f\"- Initial learning rate: {initial_lr}\\n\")\n",
    "#     f.write(f\"- Final learning rate: {summary['final_learning_rate']}\\n\")\n",
    "#     f.write(f\"- Initial action STD: {agent.init_action_std}\\n\")\n",
    "#     f.write(f\"- Final action STD: {summary['final_action_std']}\\n\")\n",
    "#     f.write(f\"- Gamma (discount factor): {agent.gamma}\\n\")\n",
    "#     f.write(f\"- GAE lambda: {agent.gae_lambda}\\n\\n\")\n",
    "    \n",
    "#     f.write(f\"## Model Files\\n\\n\")\n",
    "#     f.write(f\"- Best model: `{summary['best_model_path']}`\\n\")\n",
    "#     f.write(f\"- Final model: `{summary['final_model_path']}`\\n\\n\")\n",
    "    \n",
    "#     f.write(f\"## Performance Analysis\\n\\n\")\n",
    "#     f.write(f\"![Complete Training History](plots/complete_training_history.png)\\n\\n\")\n",
    "#     f.write(f\"![Latest Metrics](plots/latest_metrics.png)\\n\\n\")\n",
    "\n",
    "print(f\"Complete metrics saved to {metrics_dir} directory\")\n",
    "env.close()\n",
    "\n",
    "env.write_log(folder=\"logs\", file=\"nkfn77-agent-log.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small demo with a predefined heuristic that is suboptimal and has no notion of balance (and is designed for the orignal BipedalWalker environment)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.envs.box2d.bipedal_walker import BipedalWalkerHeuristics\n",
    "\n",
    "env = rld.make(\n",
    "    \"rldurham/Walker\",\n",
    "    # \"BipedalWalker-v3\",\n",
    "    render_mode=\"human\",\n",
    "    # render_mode=\"rgb_array\",\n",
    "    hardcore=False,\n",
    "    # hardcore=True,\n",
    ")\n",
    "_, obs, info = rld.seed_everything(42, env)\n",
    "\n",
    "heuristics = BipedalWalkerHeuristics()\n",
    "\n",
    "act = heuristics.step_heuristic(obs)\n",
    "for _ in range(500):\n",
    "    obs, rew, terminated, truncated, info = env.step(act)\n",
    "    act = heuristics.step_heuristic(obs)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    if env.render_mode == \"rgb_array\":\n",
    "        rld.render(env, clear=True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
