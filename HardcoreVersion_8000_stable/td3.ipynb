{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD3 Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTNU1mwGB1ZD"
   },
   "source": [
    "### Dependencies and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install swig\n",
    "# !pip install --upgrade rldurham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import rldurham as rld\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEv4ZjXmyrHo"
   },
   "source": [
    "### Prepare the environment and wrap it to capture statistics, logs, and videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Xrcek4hxDXl"
   },
   "outputs": [],
   "source": [
    "# env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\")\n",
    "env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\", hardcore=True) # only attempt this when your agent has solved the non-hardcore version\n",
    "\n",
    "# get statistics, logs, and videos\n",
    "env = rld.Recorder(\n",
    "    env,\n",
    "    smoothing=10,                       # track rolling averages (useful for plotting)\n",
    "    video=True,                         # enable recording videos\n",
    "    video_folder=\"videos\",              # folder for videos\n",
    "    video_prefix=\"xxxx00-agent-video\",  # prefix for videos (replace xxxx00 with your username)\n",
    "    logs=True,                          # keep logs\n",
    ")\n",
    "\n",
    "# training on CPU recommended\n",
    "rld.check_device()\n",
    "\n",
    "# environment info\n",
    "discrete_act, discrete_obs, act_dim, obs_dim = rld.env_info(env, print_out=True)\n",
    "\n",
    "# render start image\n",
    "env.reset(seed=42)\n",
    "rld.render(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Beta,Normal\n",
    "import math\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim, net_width, maxaction):\n",
    "\t\tsuper(Actor, self).__init__()\n",
    "\n",
    "\t\tself.l1 = nn.Linear(state_dim, net_width)\n",
    "\t\tself.l2 = nn.Linear(net_width, net_width)\n",
    "\t\tself.l3 = nn.Linear(net_width, action_dim)\n",
    "\n",
    "\t\tself.maxaction = maxaction\n",
    "\n",
    "\tdef forward(self, state):\n",
    "\t\ta = torch.tanh(self.l1(state))\n",
    "\t\ta = torch.tanh(self.l2(a))\n",
    "\t\ta = torch.tanh(self.l3(a)) * self.maxaction\n",
    "\t\treturn a\n",
    "\n",
    "\n",
    "class Q_Critic(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim, net_width):\n",
    "\t\tsuper(Q_Critic, self).__init__()\n",
    "\n",
    "\t\t# Q1 architecture\n",
    "\t\tself.l1 = nn.Linear(state_dim + action_dim, net_width)\n",
    "\t\tself.l2 = nn.Linear(net_width, net_width)\n",
    "\t\tself.l3 = nn.Linear(net_width, 1)\n",
    "\n",
    "\t\t# Q2 architecture\n",
    "\t\tself.l4 = nn.Linear(state_dim + action_dim, net_width)\n",
    "\t\tself.l5 = nn.Linear(net_width, net_width)\n",
    "\t\tself.l6 = nn.Linear(net_width, 1)\n",
    "\n",
    "\n",
    "\tdef forward(self, state, action):\n",
    "\t\tsa = torch.cat([state, action], 1)\n",
    "\n",
    "\t\tq1 = F.relu(self.l1(sa))\n",
    "\t\tq1 = F.relu(self.l2(q1))\n",
    "\t\tq1 = self.l3(q1)\n",
    "\n",
    "\t\tq2 = F.relu(self.l4(sa))\n",
    "\t\tq2 = F.relu(self.l5(q2))\n",
    "\t\tq2 = self.l6(q2)\n",
    "\t\treturn q1, q2\n",
    "\n",
    "\n",
    "\tdef Q1(self, state, action):\n",
    "\t\tsa = torch.cat([state, action], 1)\n",
    "\n",
    "\t\tq1 = F.relu(self.l1(sa))\n",
    "\t\tq1 = F.relu(self.l2(q1))\n",
    "\t\tq1 = self.l3(q1)\n",
    "\t\treturn q1\n",
    "\n",
    "\n",
    "\n",
    "class TD3(object):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tenv_with_Dead,\n",
    "\t\tstate_dim,\n",
    "\t\taction_dim,\n",
    "\t\tmax_action,\n",
    "\t\tgamma=0.99,\n",
    "\t\tnet_width=128,\n",
    "\t\ta_lr=1e-4,\n",
    "\t\tc_lr=1e-4,\n",
    "\t\tQ_batchsize = 256\n",
    "\t):\n",
    "\n",
    "\t\tself.actor = Actor(state_dim, action_dim, net_width, max_action).to(device)\n",
    "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=a_lr)\n",
    "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
    "\n",
    "\t\tself.q_critic = Q_Critic(state_dim, action_dim, net_width).to(device)\n",
    "\t\tself.q_critic_optimizer = torch.optim.Adam(self.q_critic.parameters(), lr=c_lr)\n",
    "\t\tself.q_critic_target = copy.deepcopy(self.q_critic)\n",
    "\n",
    "\t\tself.env_with_Dead = env_with_Dead\n",
    "\t\tself.action_dim = action_dim\n",
    "\t\tself.max_action = max_action\n",
    "\t\tself.gamma = gamma\n",
    "\t\tself.policy_noise = 0.2 # removed max action\n",
    "\t\tself.noise_clip = 0.5 # removed max action\n",
    "\t\tself.tau = 0.005\n",
    "\t\tself.Q_batchsize = Q_batchsize\n",
    "\t\tself.delay_counter = -1\n",
    "\t\tself.delay_freq = 2\n",
    "\n",
    "\tdef select_action(self, state):#only used when interact with the env\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tstate = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "\t\t\ta = self.actor(state)\n",
    "\t\treturn a.cpu().numpy().flatten()\n",
    "\n",
    "\tdef train(self,replay_buffer):\n",
    "\t\tself.delay_counter += 1\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\ts, a, r, s_prime, dead_mask = replay_buffer.sample(self.Q_batchsize)\n",
    "\t\t\tnoise = (torch.randn_like(a) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)\n",
    "\t\t\tsmoothed_target_a = (\n",
    "\t\t\t\t\tself.actor_target(s_prime) + noise  # Noisy on target action\n",
    "\t\t\t).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "\t\t# Compute the target Q value\n",
    "\t\ttarget_Q1, target_Q2 = self.q_critic_target(s_prime, smoothed_target_a)\n",
    "\t\ttarget_Q = torch.min(target_Q1, target_Q2)\n",
    "\t\t'''DEAD OR NOT'''\n",
    "\t\tif self.env_with_Dead:\n",
    "\t\t\ttarget_Q = r + (1 - dead_mask) * self.gamma * target_Q  # env with dead\n",
    "\t\telse:\n",
    "\t\t\ttarget_Q = r + self.gamma * target_Q  # env without dead\n",
    "\n",
    "\n",
    "\t\t# Get current Q estimates\n",
    "\t\tcurrent_Q1, current_Q2 = self.q_critic(s, a)\n",
    "\n",
    "\t\t# Compute critic loss\n",
    "\t\tq_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "\t\t# Optimize the q_critic\n",
    "\t\tself.q_critic_optimizer.zero_grad()\n",
    "\t\tq_loss.backward()\n",
    "\t\tself.q_critic_optimizer.step()\n",
    "\n",
    "\t\tif self.delay_counter == self.delay_freq:\n",
    "\t\t\t# Update Actor\n",
    "\t\t\ta_loss = -self.q_critic.Q1(s,self.actor(s)).mean()\n",
    "\t\t\tself.actor_optimizer.zero_grad()\n",
    "\t\t\ta_loss.backward()\n",
    "\t\t\tself.actor_optimizer.step()\n",
    "\n",
    "\t\t\t# Update the frozen target models\n",
    "\t\t\tfor param, target_param in zip(self.q_critic.parameters(), self.q_critic_target.parameters()):\n",
    "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\t\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\t\t\tself.delay_counter = -1\n",
    "\n",
    "\tdef save(self,episode):\n",
    "\t\ttorch.save(self.actor.state_dict(), \"actor/ppo_actor{}.pth\".format(episode))\n",
    "\t\ttorch.save(self.q_critic.state_dict(), \"critic/ppo_q_critic{}.pth\".format(episode))\n",
    "\n",
    "\n",
    "\tdef load(self,episode):\n",
    "\t\tself.actor.load_state_dict(torch.load(\"actor/ppo_actor{}.pth\".format(episode)))\n",
    "\t\tself.q_critic.load_state_dict(torch.load(\"critic/ppo_q_critic{}.pth\".format(episode)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\tdef __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "\t\tself.max_size = max_size\n",
    "\t\tself.ptr = 0\n",
    "\t\tself.size = 0\n",
    "\n",
    "\t\tself.state = np.zeros((max_size, state_dim))\n",
    "\t\tself.action = np.zeros((max_size, action_dim))\n",
    "\t\tself.reward = np.zeros((max_size, 1))\n",
    "\t\tself.next_state = np.zeros((max_size, state_dim))\n",
    "\t\tself.dead = np.zeros((max_size, 1))\n",
    "\n",
    "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\tdef add(self, state, action, reward, next_state, dead):\n",
    "\t\tself.state[self.ptr] = state\n",
    "\t\tself.action[self.ptr] = action\n",
    "\t\tself.reward[self.ptr] = reward\n",
    "\t\tself.next_state[self.ptr] = next_state\n",
    "\t\tself.dead[self.ptr] = dead #0,0,0，...，1\n",
    "\n",
    "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
    "\t\tself.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "\n",
    "\tdef sample(self, batch_size):\n",
    "\t\tind = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.action[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.dead[ind]).to(self.device)\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import rldurham as rld\n",
    "import os\n",
    "\n",
    "# Ensure folder is created for model save points\n",
    "os.makedirs(\"actor\", exist_ok=True)\n",
    "os.makedirs(\"critic\", exist_ok=True)\n",
    "\n",
    "# Training on CPU recommended\n",
    "rld.check_device()\n",
    "\n",
    "# Environment info\n",
    "discrete_act, discrete_obs, act_dim, obs_dim = rld.env_info(env, print_out=True)\n",
    "\n",
    "# in the submission please use seed_everything with seed 42 for verification\n",
    "seed, observation, info = rld.seed_everything(42, env)\n",
    "\n",
    "# Render start image\n",
    "rld.render(env)\n",
    "\n",
    "# Check if the environment has continuous action space\n",
    "env_with_Dead = True  # Whether the Env has dead state\n",
    "state_dim = obs_dim\n",
    "action_dim = act_dim\n",
    "max_action = 1.0  # Walker environment typically has actions scaled to [-1, 1]\n",
    "\n",
    "print(' state_dim:', state_dim, ' action_dim:', action_dim, \n",
    "        ' max_a:', max_action, ' min_a:', -max_action)\n",
    "\n",
    "# Training parameters\n",
    "max_episodes = 8000\n",
    "max_timesteps = 2000\n",
    "save_interval = 100\n",
    "expl_noise = 0.25\n",
    "\n",
    "# Set random seeds (already set by seed_everything but keeping for clarity)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Create the agent\n",
    "kwargs = {\n",
    "    \"env_with_Dead\": env_with_Dead,\n",
    "    \"state_dim\": state_dim,\n",
    "    \"action_dim\": action_dim,\n",
    "    \"max_action\": max_action,\n",
    "    \"gamma\": 0.99,\n",
    "    \"net_width\": 256,\n",
    "    \"a_lr\": 1e-4,\n",
    "    \"c_lr\": 1e-4,\n",
    "    \"Q_batchsize\": 256,\n",
    "}\n",
    "\n",
    "agent = TD3(**kwargs)\n",
    "\n",
    "# Create replay buffer\n",
    "replay_buffer = ReplayBuffer(state_dim, action_dim, max_size=int(9e4)) # changed from 1e6 to see if memory issues are better\n",
    "\n",
    "# Track statistics for plotting\n",
    "tracker = rld.InfoTracker()\n",
    "all_ep_r = []\n",
    "\n",
    "# Training procedure\n",
    "for episode in range(max_episodes):\n",
    "    # Recording statistics and video can be switched on and off (video recording is slow!)\n",
    "    env.info = episode % 10 == 0  # track every x episodes\n",
    "    env.video = episode % 10 == 0  # record videos every x episodes\n",
    "    \n",
    "    # Reset for new episode\n",
    "    observation, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    # Decay exploration noise\n",
    "    expl_noise *= 0.999\n",
    "    \n",
    "    # Episode loop\n",
    "    for t in range(max_timesteps):\n",
    "        steps += 1\n",
    "        \n",
    "        # Select action with exploration noise\n",
    "        # action = (\n",
    "        #     agent.select_action(observation) + \n",
    "        #     np.random.normal(0, max_action * expl_noise, size=action_dim)\n",
    "        # ).clip(-max_action, max_action)\n",
    "        \n",
    "        \n",
    "        # Take action in the environment\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Accumulate episode reward\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # Check whether done\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Special reward handling for Walker environments\n",
    "        if reward <= -100 and env_with_Dead:\n",
    "            reward = -1\n",
    "            replay_buffer.add(observation, action, reward, next_observation, True)\n",
    "        else:\n",
    "            replay_buffer.add(observation, action, reward, next_observation, done)\n",
    "        \n",
    "        # Update current observation\n",
    "        observation = next_observation\n",
    "        \n",
    "        # Train the agent if buffer has enough samples\n",
    "        if replay_buffer.size > 2000:\n",
    "            agent.train(replay_buffer)\n",
    "        \n",
    "        # Stop episode if done\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Save model at specified intervals\n",
    "    if (episode + 1) % save_interval == 0:\n",
    "        agent.save(episode + 1)\n",
    "    \n",
    "    # Record and log statistics\n",
    "    if episode == 0:\n",
    "        all_ep_r.append(episode_reward)\n",
    "    else:\n",
    "        all_ep_r.append(all_ep_r[-1] * 0.9 + episode_reward * 0.1)\n",
    "    \n",
    "    # Track and plot statistics\n",
    "    tracker.track(info)\n",
    "    \n",
    "    if (episode + 1) % 10 == 0:\n",
    "        tracker.plot(r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "        print('seed:', seed, 'episode:', episode, 'score:', episode_reward, \n",
    "                'step:', steps, 'smoothed score:', all_ep_r[-1])\n",
    "\n",
    "# Don't forget to close environment (e.g. triggers last video save)\n",
    "env.close()\n",
    "\n",
    "# Write log file (for coursework)\n",
    "env.write_log(folder=\"logs\", file=\"nkfn77-agent-log.txt\")  # replace with your username\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small demo with a predefined heuristic that is suboptimal and has no notion of balance (and is designed for the orignal BipedalWalker environment)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.box2d.bipedal_walker import BipedalWalkerHeuristics\n",
    "\n",
    "env = rld.make(\n",
    "    \"rldurham/Walker\",\n",
    "    # \"BipedalWalker-v3\",\n",
    "    render_mode=\"human\",\n",
    "    # render_mode=\"rgb_array\",\n",
    "    hardcore=False,\n",
    "    # hardcore=True,\n",
    ")\n",
    "_, obs, info = rld.seed_everything(42, env)\n",
    "\n",
    "heuristics = BipedalWalkerHeuristics()\n",
    "\n",
    "act = heuristics.step_heuristic(obs)\n",
    "for _ in range(500):\n",
    "    obs, rew, terminated, truncated, info = env.step(act)\n",
    "    act = heuristics.step_heuristic(obs)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    if env.render_mode == \"rgb_array\":\n",
    "        rld.render(env, clear=True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
