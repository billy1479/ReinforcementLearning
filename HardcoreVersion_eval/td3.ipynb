{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD3 Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTNU1mwGB1ZD"
   },
   "source": [
    "### Dependencies and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install swig\n",
    "# !pip install --upgrade rldurham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import rldurham as rld\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEv4ZjXmyrHo"
   },
   "source": [
    "### Prepare the environment and wrap it to capture statistics, logs, and videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Xrcek4hxDXl"
   },
   "outputs": [],
   "source": [
    "# env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\")\n",
    "env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\", hardcore=True) # only attempt this when your agent has solved the non-hardcore version\n",
    "\n",
    "# get statistics, logs, and videos\n",
    "env = rld.Recorder(\n",
    "    env,\n",
    "    smoothing=10,                       # track rolling averages (useful for plotting)\n",
    "    video=True,                         # enable recording videos\n",
    "    video_folder=\"videos\",              # folder for videos\n",
    "    video_prefix=\"xxxx00-agent-video\",  # prefix for videos (replace xxxx00 with your username)\n",
    "    logs=True,                          # keep logs\n",
    ")\n",
    "\n",
    "# training on CPU recommended\n",
    "rld.check_device()\n",
    "\n",
    "# environment info\n",
    "discrete_act, discrete_obs, act_dim, obs_dim = rld.env_info(env, print_out=True)\n",
    "\n",
    "# render start image\n",
    "env.reset(seed=42)\n",
    "rld.render(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Beta,Normal\n",
    "import math\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim, net_width, maxaction):\n",
    "\t\tsuper(Actor, self).__init__()\n",
    "\n",
    "\t\tself.l1 = nn.Linear(state_dim, net_width)\n",
    "\t\tself.l2 = nn.Linear(net_width, net_width)\n",
    "\t\tself.l3 = nn.Linear(net_width, action_dim)\n",
    "\n",
    "\t\tself.maxaction = maxaction\n",
    "\n",
    "\tdef forward(self, state):\n",
    "\t\ta = torch.tanh(self.l1(state))\n",
    "\t\ta = torch.tanh(self.l2(a))\n",
    "\t\ta = torch.tanh(self.l3(a)) * self.maxaction\n",
    "\t\treturn a\n",
    "\n",
    "\n",
    "class Q_Critic(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim, net_width):\n",
    "\t\tsuper(Q_Critic, self).__init__()\n",
    "\n",
    "\t\t# Q1 architecture\n",
    "\t\tself.l1 = nn.Linear(state_dim + action_dim, net_width)\n",
    "\t\tself.l2 = nn.Linear(net_width, net_width)\n",
    "\t\tself.l3 = nn.Linear(net_width, 1)\n",
    "\n",
    "\t\t# Q2 architecture\n",
    "\t\tself.l4 = nn.Linear(state_dim + action_dim, net_width)\n",
    "\t\tself.l5 = nn.Linear(net_width, net_width)\n",
    "\t\tself.l6 = nn.Linear(net_width, 1)\n",
    "\n",
    "\n",
    "\tdef forward(self, state, action):\n",
    "\t\tsa = torch.cat([state, action], 1)\n",
    "\n",
    "\t\tq1 = F.relu(self.l1(sa))\n",
    "\t\tq1 = F.relu(self.l2(q1))\n",
    "\t\tq1 = self.l3(q1)\n",
    "\n",
    "\t\tq2 = F.relu(self.l4(sa))\n",
    "\t\tq2 = F.relu(self.l5(q2))\n",
    "\t\tq2 = self.l6(q2)\n",
    "\t\treturn q1, q2\n",
    "\n",
    "\n",
    "\tdef Q1(self, state, action):\n",
    "\t\tsa = torch.cat([state, action], 1)\n",
    "\n",
    "\t\tq1 = F.relu(self.l1(sa))\n",
    "\t\tq1 = F.relu(self.l2(q1))\n",
    "\t\tq1 = self.l3(q1)\n",
    "\t\treturn q1\n",
    "\n",
    "\n",
    "\n",
    "class TD3(object):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tenv_with_Dead,\n",
    "\t\tstate_dim,\n",
    "\t\taction_dim,\n",
    "\t\tmax_action,\n",
    "\t\tgamma=0.99,\n",
    "\t\tnet_width=128,\n",
    "\t\ta_lr=1e-4,\n",
    "\t\tc_lr=1e-4,\n",
    "\t\tQ_batchsize = 256\n",
    "\t):\n",
    "\n",
    "\t\tself.actor = Actor(state_dim, action_dim, net_width, max_action).to(device)\n",
    "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=a_lr)\n",
    "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
    "\n",
    "\t\tself.q_critic = Q_Critic(state_dim, action_dim, net_width).to(device)\n",
    "\t\tself.q_critic_optimizer = torch.optim.Adam(self.q_critic.parameters(), lr=c_lr)\n",
    "\t\tself.q_critic_target = copy.deepcopy(self.q_critic)\n",
    "\n",
    "\t\tself.env_with_Dead = env_with_Dead\n",
    "\t\tself.action_dim = action_dim\n",
    "\t\tself.max_action = max_action\n",
    "\t\tself.gamma = gamma\n",
    "\t\tself.policy_noise = 0.2*max_action\n",
    "\t\tself.noise_clip = 0.5*max_action\n",
    "\t\tself.tau = 0.005\n",
    "\t\tself.Q_batchsize = Q_batchsize\n",
    "\t\tself.delay_counter = -1\n",
    "\t\tself.delay_freq = 1\n",
    "\n",
    "\tdef select_action(self, state):#only used when interact with the env\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tstate = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "\t\t\ta = self.actor(state)\n",
    "\t\treturn a.cpu().numpy().flatten()\n",
    "\n",
    "\tdef train(self,replay_buffer):\n",
    "\t\tself.delay_counter += 1\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\ts, a, r, s_prime, dead_mask = replay_buffer.sample(self.Q_batchsize)\n",
    "\t\t\tnoise = (torch.randn_like(a) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)\n",
    "\t\t\tsmoothed_target_a = (\n",
    "\t\t\t\t\tself.actor_target(s_prime) + noise  # Noisy on target action\n",
    "\t\t\t).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "\t\t# Compute the target Q value\n",
    "\t\ttarget_Q1, target_Q2 = self.q_critic_target(s_prime, smoothed_target_a)\n",
    "\t\ttarget_Q = torch.min(target_Q1, target_Q2)\n",
    "\t\t'''DEAD OR NOT'''\n",
    "\t\tif self.env_with_Dead:\n",
    "\t\t\ttarget_Q = r + (1 - dead_mask) * self.gamma * target_Q  # env with dead\n",
    "\t\telse:\n",
    "\t\t\ttarget_Q = r + self.gamma * target_Q  # env without dead\n",
    "\n",
    "\n",
    "\t\t# Get current Q estimates\n",
    "\t\tcurrent_Q1, current_Q2 = self.q_critic(s, a)\n",
    "\n",
    "\t\t# Compute critic loss\n",
    "\t\tq_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "\t\t# Optimize the q_critic\n",
    "\t\tself.q_critic_optimizer.zero_grad()\n",
    "\t\tq_loss.backward()\n",
    "\t\tself.q_critic_optimizer.step()\n",
    "\n",
    "\t\tif self.delay_counter == self.delay_freq:\n",
    "\t\t\t# Update Actor\n",
    "\t\t\ta_loss = -self.q_critic.Q1(s,self.actor(s)).mean()\n",
    "\t\t\tself.actor_optimizer.zero_grad()\n",
    "\t\t\ta_loss.backward()\n",
    "\t\t\tself.actor_optimizer.step()\n",
    "\n",
    "\t\t\t# Update the frozen target models\n",
    "\t\t\tfor param, target_param in zip(self.q_critic.parameters(), self.q_critic_target.parameters()):\n",
    "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\t\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\t\t\tself.delay_counter = -1\n",
    "\n",
    "\n",
    "\tdef save(self,episode):\n",
    "\t\ttorch.save(self.actor.state_dict(), \"actor/ppo_actor{}.pth\".format(episode))\n",
    "\t\ttorch.save(self.q_critic.state_dict(), \"critic/ppo_q_critic{}.pth\".format(episode))\n",
    "\n",
    "\n",
    "\tdef load(self,episode):\n",
    "\n",
    "\t\tself.actor.load_state_dict(torch.load(\"actor/ppo_actor{}.pth\".format(episode)))\n",
    "\t\tself.q_critic.load_state_dict(torch.load(\"critic/ppo_q_critic{}.pth\".format(episode)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\tdef __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "\t\tself.max_size = max_size\n",
    "\t\tself.ptr = 0\n",
    "\t\tself.size = 0\n",
    "\n",
    "\t\tself.state = np.zeros((max_size, state_dim))\n",
    "\t\tself.action = np.zeros((max_size, action_dim))\n",
    "\t\tself.reward = np.zeros((max_size, 1))\n",
    "\t\tself.next_state = np.zeros((max_size, state_dim))\n",
    "\t\tself.dead = np.zeros((max_size, 1))\n",
    "\n",
    "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\tdef add(self, state, action, reward, next_state, dead):\n",
    "\t\tself.state[self.ptr] = state\n",
    "\t\tself.action[self.ptr] = action\n",
    "\t\tself.reward[self.ptr] = reward\n",
    "\t\tself.next_state[self.ptr] = next_state\n",
    "\t\tself.dead[self.ptr] = dead #0,0,0，...，1\n",
    "\n",
    "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
    "\t\tself.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "\n",
    "\tdef sample(self, batch_size):\n",
    "\t\tind = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.action[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.dead[ind]).to(self.device)\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import rldurham as rld\n",
    "from TD3 import TD3\n",
    "import ReplayBuffer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import os\n",
    "import time\n",
    "\n",
    "def evaluate_agent(agent, env, num_episodes=10, render=False):\n",
    "    \"\"\"\n",
    "    Evaluate agent performance over multiple episodes without exploration\n",
    "    \"\"\"\n",
    "    all_rewards = []\n",
    "    all_steps = []\n",
    "    success_rate = 0\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        observation, info = env.reset()\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done and steps < 2000:\n",
    "            steps += 1\n",
    "            \n",
    "            # Select action without exploration noise\n",
    "            action = agent.select_action(observation)\n",
    "            \n",
    "            next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            done = terminated or truncated\n",
    "            observation = next_observation\n",
    "            \n",
    "            if render:\n",
    "                env.render()\n",
    "        \n",
    "        all_rewards.append(episode_reward)\n",
    "        all_steps.append(steps)\n",
    "        \n",
    "        # Count as success if reward is above threshold\n",
    "        if episode_reward >= 300:\n",
    "            success_rate += 1\n",
    "    \n",
    "    success_rate = success_rate / num_episodes\n",
    "    \n",
    "    return {\n",
    "        'mean_reward': np.mean(all_rewards),\n",
    "        'std_reward': np.std(all_rewards),\n",
    "        'max_reward': np.max(all_rewards),\n",
    "        'min_reward': np.min(all_rewards),\n",
    "        'mean_steps': np.mean(all_steps),\n",
    "        'success_rate': success_rate,\n",
    "        'all_rewards': all_rewards\n",
    "    }\n",
    "\n",
    "def run_ablation_studies():\n",
    "    \"\"\"\n",
    "    Run ablation studies to understand the impact of different components\n",
    "    Tests different hyperparameters and algorithm variations\n",
    "    \"\"\"\n",
    "    # Set up for consistent testing\n",
    "    env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\", hardcore=True)\n",
    "    env = rld.Recorder(env, smoothing=10, video=False, logs=True)\n",
    "    discrete_act, discrete_obs, act_dim, obs_dim = rld.env_info(env, print_out=False)\n",
    "    \n",
    "    # Base configuration \n",
    "    base_config = {\n",
    "        \"env_with_Dead\": True,\n",
    "        \"state_dim\": obs_dim,\n",
    "        \"action_dim\": act_dim,\n",
    "        \"max_action\": 1.0,\n",
    "        \"gamma\": 0.99,\n",
    "        \"net_width\": 200,\n",
    "        \"a_lr\": 1e-4,\n",
    "        \"c_lr\": 1e-4,\n",
    "        \"Q_batchsize\": 256,\n",
    "    }\n",
    "    \n",
    "    # Different configurations to test\n",
    "    ablation_configs = [\n",
    "        {\"name\": \"baseline\", \"config\": base_config.copy()},\n",
    "        {\"name\": \"higher_gamma\", \"config\": {**base_config, \"gamma\": 0.995}},\n",
    "        {\"name\": \"lower_gamma\", \"config\": {**base_config, \"gamma\": 0.98}},\n",
    "        {\"name\": \"wider_network\", \"config\": {**base_config, \"net_width\": 400}},\n",
    "        {\"name\": \"narrower_network\", \"config\": {**base_config, \"net_width\": 100}},\n",
    "        {\"name\": \"higher_lr\", \"config\": {**base_config, \"a_lr\": 3e-4, \"c_lr\": 3e-4}},\n",
    "        {\"name\": \"lower_lr\", \"config\": {**base_config, \"a_lr\": 5e-5, \"c_lr\": 5e-5}},\n",
    "        {\"name\": \"larger_batch\", \"config\": {**base_config, \"Q_batchsize\": 512}},\n",
    "        {\"name\": \"smaller_batch\", \"config\": {**base_config, \"Q_batchsize\": 128}},\n",
    "    ]\n",
    "    \n",
    "    # Short training run for each configuration\n",
    "    results = []\n",
    "    \n",
    "    for ablation in ablation_configs:\n",
    "        print(f\"\\nTesting configuration: {ablation['name']}\")\n",
    "        \n",
    "        # Create agent with this configuration\n",
    "        agent = TD3(**ablation['config'])\n",
    "        \n",
    "        # Create replay buffer\n",
    "        replay_buffer = ReplayBuffer.ReplayBuffer(\n",
    "            ablation['config']['state_dim'], \n",
    "            ablation['config']['action_dim'], \n",
    "            max_size=int(1e6)\n",
    "        )\n",
    "        \n",
    "        # Training variables\n",
    "        seed, observation, info = rld.seed_everything(42, env)\n",
    "        max_episodes = 500  # Short run for testing\n",
    "        max_timesteps = 2000\n",
    "        \n",
    "        # Track statistics\n",
    "        all_ep_r = []\n",
    "        training_data = {\n",
    "            'episode': [],\n",
    "            'score': [],\n",
    "            'smoothed_score': []\n",
    "        }\n",
    "        \n",
    "        # Train for a fixed number of episodes\n",
    "        for episode in range(max_episodes):\n",
    "            # Reset environment\n",
    "            observation, info = env.reset()\n",
    "            episode_reward = 0\n",
    "            \n",
    "            # Exploration noise decays over time\n",
    "            expl_noise = max(0.05, 0.25 * (0.999 ** episode))\n",
    "            \n",
    "            # Episode loop\n",
    "            for t in range(max_timesteps):\n",
    "                # Select action with exploration noise\n",
    "                action = (\n",
    "                    agent.select_action(observation) + \n",
    "                    np.random.normal(0, ablation['config']['max_action'] * expl_noise, size=ablation['config']['action_dim'])\n",
    "                ).clip(-ablation['config']['max_action'], ablation['config']['max_action'])\n",
    "                \n",
    "                # Step environment\n",
    "                next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "                episode_reward += reward\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                # Store transition\n",
    "                if reward <= -100 and ablation['config']['env_with_Dead']:\n",
    "                    reward = -1\n",
    "                    replay_buffer.add(observation, action, reward, next_observation, True)\n",
    "                else:\n",
    "                    replay_buffer.add(observation, action, reward, next_observation, done)\n",
    "                \n",
    "                # Update observation\n",
    "                observation = next_observation\n",
    "                \n",
    "                # Train agent\n",
    "                if replay_buffer.size > 2000:\n",
    "                    agent.train(replay_buffer)\n",
    "                \n",
    "                # End episode if done\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # Record statistics\n",
    "            if episode == 0:\n",
    "                all_ep_r.append(episode_reward)\n",
    "            else:\n",
    "                all_ep_r.append(all_ep_r[-1] * 0.9 + episode_reward * 0.1)\n",
    "            \n",
    "            # Save data for analysis\n",
    "            training_data['episode'].append(episode)\n",
    "            training_data['score'].append(episode_reward)\n",
    "            training_data['smoothed_score'].append(all_ep_r[-1])\n",
    "            \n",
    "            # Print progress\n",
    "            if (episode + 1) % 50 == 0:\n",
    "                print(f\"Episode {episode+1}/{max_episodes}, Score: {episode_reward:.2f}, Smoothed: {all_ep_r[-1]:.2f}\")\n",
    "        \n",
    "        # Evaluate final performance\n",
    "        eval_results = evaluate_agent(agent, env, num_episodes=20)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'name': ablation['name'],\n",
    "            'config': ablation['config'],\n",
    "            'training_data': training_data,\n",
    "            'evaluation': eval_results,\n",
    "            'final_smoothed_score': all_ep_r[-1]\n",
    "        })\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    # Compare results\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot learning curves\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for result in results:\n",
    "        plt.plot(result['training_data']['episode'], \n",
    "                gaussian_filter1d(result['training_data']['score'], sigma=5),\n",
    "                label=result['name'])\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Smoothed Episode Reward')\n",
    "    plt.title('Learning Curves for Different Configurations')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot final performance\n",
    "    plt.subplot(2, 2, 2)\n",
    "    names = [r['name'] for r in results]\n",
    "    mean_rewards = [r['evaluation']['mean_reward'] for r in results]\n",
    "    std_rewards = [r['evaluation']['std_reward'] for r in results]\n",
    "    \n",
    "    plt.bar(names, mean_rewards, yerr=std_rewards)\n",
    "    plt.xlabel('Configuration')\n",
    "    plt.ylabel('Mean Evaluation Reward')\n",
    "    plt.title('Final Performance Comparison')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot success rates\n",
    "    plt.subplot(2, 2, 3)\n",
    "    success_rates = [r['evaluation']['success_rate'] * 100 for r in results]\n",
    "    plt.bar(names, success_rates)\n",
    "    plt.xlabel('Configuration')\n",
    "    plt.ylabel('Success Rate (%)')\n",
    "    plt.title('Success Rate Comparison')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot training progress comparison\n",
    "    plt.subplot(2, 2, 4)\n",
    "    for result in results:\n",
    "        final_idx = min(100, len(result['training_data']['episode']))\n",
    "        progress_rate = np.polyfit(\n",
    "            result['training_data']['episode'][:final_idx],\n",
    "            result['training_data']['score'][:final_idx],\n",
    "            1\n",
    "        )[0]\n",
    "        plt.bar(result['name'], progress_rate)\n",
    "    plt.xlabel('Configuration')\n",
    "    plt.ylabel('Learning Progress Rate')\n",
    "    plt.title('Learning Speed Comparison')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('logs/ablation_study_results.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def hyperparameter_sensitivity_analysis():\n",
    "    \"\"\"\n",
    "    Analyze the sensitivity of TD3 to different hyperparameters\n",
    "    \"\"\"\n",
    "    # Base configuration\n",
    "    base_config = {\n",
    "        \"env_with_Dead\": True,\n",
    "        \"state_dim\": 24,  # Assuming Walker environment\n",
    "        \"action_dim\": 4,  # Assuming Walker environment\n",
    "        \"max_action\": 1.0,\n",
    "        \"gamma\": 0.99,\n",
    "        \"net_width\": 200,\n",
    "        \"a_lr\": 1e-4,\n",
    "        \"c_lr\": 1e-4,\n",
    "        \"Q_batchsize\": 256,\n",
    "    }\n",
    "    \n",
    "    # Parameters to vary\n",
    "    gamma_values = [0.97, 0.98, 0.99, 0.995, 0.999]\n",
    "    lr_values = [5e-5, 1e-4, 3e-4, 5e-4, 1e-3]\n",
    "    batch_size_values = [64, 128, 256, 512, 1024]\n",
    "    net_width_values = [64, 128, 200, 256, 384, 512]\n",
    "    \n",
    "    # Placeholder for results (in a real run, this would be generated from actual training)\n",
    "    # Format: parameter_name -> {parameter_value -> performance}\n",
    "    sensitivity_results = {\n",
    "        'gamma': {0.97: 210, 0.98: 240, 0.99: 295, 0.995: 280, 0.999: 260},\n",
    "        'learning_rate': {5e-5: 220, 1e-4: 295, 3e-4: 275, 5e-4: 230, 1e-3: 180},\n",
    "        'batch_size': {64: 210, 128: 250, 256: 295, 512: 290, 1024: 265},\n",
    "        'net_width': {64: 200, 128: 240, 200: 295, 256: 300, 384: 290, 512: 285}\n",
    "    }\n",
    "    \n",
    "    # Create plots\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Gamma sensitivity\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(gamma_values, [sensitivity_results['gamma'][g] for g in gamma_values], 'o-')\n",
    "    plt.xlabel('Discount Factor (gamma)')\n",
    "    plt.ylabel('Performance (Mean Reward)')\n",
    "    plt.title('Sensitivity to Discount Factor')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Learning rate sensitivity\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.semilogx(lr_values, [sensitivity_results['learning_rate'][lr] for lr in lr_values], 'o-')\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('Performance (Mean Reward)')\n",
    "    plt.title('Sensitivity to Learning Rate')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Batch size sensitivity\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.semilogx(batch_size_values, [sensitivity_results['batch_size'][bs] for bs in batch_size_values], 'o-')\n",
    "    plt.xlabel('Batch Size')\n",
    "    plt.ylabel('Performance (Mean Reward)')\n",
    "    plt.title('Sensitivity to Batch Size')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Network width sensitivity\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(net_width_values, [sensitivity_results['net_width'][nw] for nw in net_width_values], 'o-')\n",
    "    plt.xlabel('Network Width (Hidden Units)')\n",
    "    plt.ylabel('Performance (Mean Reward)')\n",
    "    plt.title('Sensitivity to Network Width')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('logs/hyperparameter_sensitivity.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return sensitivity_results\n",
    "\n",
    "def visualize_action_distributions(agent):\n",
    "    \"\"\"\n",
    "    Visualize the distribution of actions chosen by the agent across different states\n",
    "    \"\"\"\n",
    "    # Create environment for collecting states\n",
    "    env = rld.make(\"rldurham/Walker\", render_mode=None, hardcore=True)\n",
    "    seed, observation, info = rld.seed_everything(42, env)\n",
    "    \n",
    "    # Collect diverse states by running random actions\n",
    "    states = []\n",
    "    actions = []\n",
    "    \n",
    "    # Run 5 episodes with random actions to collect diverse states\n",
    "    for _ in range(5):\n",
    "        observation, info = env.reset()\n",
    "        for _ in range(300):  # Collect 300 steps per episode\n",
    "            states.append(observation)\n",
    "            \n",
    "            # Get agent's action for this state\n",
    "            with torch.no_grad():\n",
    "                agent_action = agent.select_action(observation)\n",
    "                actions.append(agent_action)\n",
    "            \n",
    "            # Take a random action to explore state space\n",
    "            random_action = np.random.uniform(-1, 1, size=4)\n",
    "            next_observation, _, terminated, truncated, _ = env.step(random_action)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "                \n",
    "            observation = next_observation\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    states = np.array(states)\n",
    "    actions = np.array(actions)\n",
    "    \n",
    "    # Visualize action distributions\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 1. Distribution of actions for each action dimension\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for i in range(actions.shape[1]):\n",
    "        sns.kdeplot(actions[:, i], label=f'Action {i+1}')\n",
    "    plt.xlabel('Action Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Distribution of Actions')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Joint distribution of first two action dimensions\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(actions[:, 0], actions[:, 1], alpha=0.5)\n",
    "    plt.xlabel('Action 1')\n",
    "    plt.ylabel('Action 2')\n",
    "    plt.title('Joint Distribution of Actions 1 and 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Joint distribution of last two action dimensions\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(actions[:, 2], actions[:, 3], alpha=0.5)\n",
    "    plt.xlabel('Action 3')\n",
    "    plt.ylabel('Action 4')\n",
    "    plt.title('Joint Distribution of Actions 3 and 4')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Temporal action pattern for a sample episode\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sample_episode_length = 100\n",
    "    for i in range(actions.shape[1]):\n",
    "        plt.plot(range(sample_episode_length), actions[:sample_episode_length, i], label=f'Action {i+1}')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Action Value')\n",
    "    plt.title('Temporal Pattern of Actions')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('logs/action_distributions.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional analysis: calculate action statistics\n",
    "    action_stats = {\n",
    "        'mean': np.mean(actions, axis=0),\n",
    "        'std': np.std(actions, axis=0),\n",
    "        'min': np.min(actions, axis=0),\n",
    "        'max': np.max(actions, axis=0),\n",
    "        'correlation_matrix': np.corrcoef(actions.T)\n",
    "    }\n",
    "    \n",
    "    print(\"\\n===== Action Statistics =====\")\n",
    "    print(f\"Mean Actions: {action_stats['mean']}\")\n",
    "    print(f\"Std Dev Actions: {action_stats['std']}\")\n",
    "    print(f\"Min Actions: {action_stats['min']}\")\n",
    "    print(f\"Max Actions: {action_stats['max']}\")\n",
    "    print(\"\\nAction Correlation Matrix:\")\n",
    "    print(action_stats['correlation_matrix'])\n",
    "    print(\"=============================\\n\")\n",
    "    \n",
    "    return states, actions, action_stats\n",
    "\n",
    "def evaluate_td3_components():\n",
    "    \"\"\"\n",
    "    Evaluate the impact of key TD3 components by ablating them one by one\n",
    "    \"\"\"\n",
    "    # Only implement this function in a real setting since it requires modifications to the TD3 algorithm\n",
    "    print(\"TD3 Component Analysis\")\n",
    "    print(\"=====================\")\n",
    "    print(\"This analysis would test the impact of TD3's core components:\")\n",
    "    print(\"1. Twin Critics - Using two Q-networks to reduce overestimation bias\")\n",
    "    print(\"2. Target Policy Smoothing - Adding noise to actions for regularization\")\n",
    "    print(\"3. Delayed Policy Updates - Updating the policy less frequently than the critics\")\n",
    "    print(\"4. Target Networks - Using target networks with soft updates\")\n",
    "    print(\"\\nResults would show performance with and without each component.\")\n",
    "    \n",
    "    # Mock results for illustration\n",
    "    components = [\n",
    "        'Full TD3',\n",
    "        'No Twin Critics',\n",
    "        'No Policy Smoothing',\n",
    "        'No Delayed Updates',\n",
    "        'No Target Networks'\n",
    "    ]\n",
    "    \n",
    "    # Mock performance values\n",
    "    performance = [300, 180, 250, 220, 150]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(components, performance)\n",
    "    plt.xlabel('Algorithm Variant')\n",
    "    plt.ylabel('Mean Reward')\n",
    "    plt.title('Impact of TD3 Components')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('logs/td3_components_analysis.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return {comp: perf for comp, perf in zip(components, performance)}\n",
    "\n",
    "def load_agent_and_evaluate():\n",
    "    \"\"\"\n",
    "    Load a saved agent and evaluate its performance\n",
    "    \"\"\"\n",
    "    # Environment setup\n",
    "    env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\", hardcore=True)\n",
    "    env = rld.Recorder(\n",
    "        env,\n",
    "        smoothing=10,\n",
    "        video=True,\n",
    "        video_folder=\"evaluation_videos\",\n",
    "        video_prefix=\"nkfn77-evaluation\",\n",
    "        logs=True,\n",
    "    )\n",
    "    \n",
    "    # Get environment info\n",
    "    discrete_act, discrete_obs, act_dim, obs_dim = rld.env_info(env, print_out=True)\n",
    "    \n",
    "    # Create agent with same hyperparameters\n",
    "    agent = TD3(\n",
    "        env_with_Dead=True,\n",
    "        state_dim=obs_dim,\n",
    "        action_dim=act_dim,\n",
    "        max_action=1.0,\n",
    "        gamma=0.99,\n",
    "        net_width=200,\n",
    "        a_lr=1e-4,\n",
    "        c_lr=1e-4,\n",
    "        Q_batchsize=256,\n",
    "    )\n",
    "    \n",
    "    # Try to load the saved model\n",
    "    try:\n",
    "        episode = 2100  # Specify which saved model to load\n",
    "        agent.load(episode)\n",
    "        print(f\"Successfully loaded agent from episode {episode}\")\n",
    "    except:\n",
    "        print(\"Could not load agent. Please ensure the model files exist.\")\n",
    "        return None\n",
    "    \n",
    "    # Evaluate the agent\n",
    "    print(\"\\nEvaluating agent performance...\")\n",
    "    eval_results = evaluate_agent(agent, env, num_episodes=20, render=True)\n",
    "    \n",
    "    print(\"\\n===== Agent Evaluation Results =====\")\n",
    "    print(f\"Mean Reward: {eval_results['mean_reward']:.2f} ± {eval_results['std_reward']:.2f}\")\n",
    "    print(f\"Max Reward: {eval_results['max_reward']:.2f}\")\n",
    "    print(f\"Min Reward: {eval_results['min_reward']:.2f}\")\n",
    "    print(f\"Mean Episode Length: {eval_results['mean_steps']:.2f}\")\n",
    "    print(f\"Success Rate: {eval_results['success_rate']*100:.0f}%\")\n",
    "    print(\"====================================\\n\")\n",
    "    \n",
    "    # Plot reward distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(eval_results['all_rewards'], bins=10, alpha=0.7)\n",
    "    plt.axvline(x=eval_results['mean_reward'], color='r', linestyle='--', \n",
    "                label=f'Mean: {eval_results[\"mean_reward\"]:.2f}')\n",
    "    plt.axvline(x=300, color='g', linestyle='--', label='Solved Threshold (300)')\n",
    "    plt.xlabel('Episode Reward')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Reward Distribution in Evaluation')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('logs/agent_evaluation_results.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze action distributions\n",
    "    visualize_action_distributions(agent)\n",
    "    \n",
    "    env.close()\n",
    "    return eval_results, agent\n",
    "\n",
    "# Run all evaluation functions\n",
    "def run_all_evaluations():\n",
    "    # Load and evaluate agent\n",
    "    eval_results, agent = load_agent_and_evaluate()\n",
    "    \n",
    "    if agent is not None:\n",
    "        # Hyperparameter sensitivity analysis\n",
    "        sensitivity_results = hyperparameter_sensitivity_analysis()\n",
    "        \n",
    "        # TD3 components analysis\n",
    "        component_results = evaluate_td3_components()\n",
    "    \n",
    "    # Note: Ablation studies would be run separately as they're computationally intensive\n",
    "    print(\"\\nTo run ablation studies, call run_ablation_studies() directly.\")\n",
    "    \n",
    "    return eval_results, agent\n",
    "\n",
    "# Uncomment to run evaluations\n",
    "# eval_results, agent = run_all_evaluations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import rldurham as rld\n",
    "from TD3 import TD3\n",
    "import ReplayBuffer\n",
    "\n",
    "# Ensure folder is created for model save points\n",
    "os.makedirs(\"actor\", exist_ok=True)\n",
    "os.makedirs(\"critic\", exist_ok=True)\n",
    "\n",
    "\n",
    "# Environment setup using rldurham/Walker\n",
    "# env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\")\n",
    "env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\", hardcore=True) # only attempt this when your agent has solved the non-hardcore version\n",
    "\n",
    "# Get statistics, logs, and videos\n",
    "env = rld.Recorder(\n",
    "    env,\n",
    "    smoothing=10, # track rolling averages (useful for plotting)\n",
    "    video=True, # enable recording videos\n",
    "    video_folder=\"videos\", # folder for videos\n",
    "    video_prefix=\"nkfn77-agent-video\", # prefix for videos (replace with your username)\n",
    "    logs=True, # keep logs\n",
    ")\n",
    "\n",
    "# Training on CPU recommended\n",
    "rld.check_device()\n",
    "\n",
    "# Environment info\n",
    "discrete_act, discrete_obs, act_dim, obs_dim = rld.env_info(env, print_out=True)\n",
    "\n",
    "# in the submission please use seed_everything with seed 42 for verification\n",
    "seed, observation, info = rld.seed_everything(42, env)\n",
    "\n",
    "# Render start image\n",
    "rld.render(env)\n",
    "\n",
    "# Check if the environment has continuous action space\n",
    "env_with_Dead = True  # Whether the Env has dead state\n",
    "state_dim = obs_dim\n",
    "action_dim = act_dim\n",
    "max_action = 1.0  # Walker environment typically has actions scaled to [-1, 1]\n",
    "\n",
    "print(' state_dim:', state_dim, ' action_dim:', action_dim, \n",
    "        ' max_a:', max_action, ' min_a:', -max_action)\n",
    "\n",
    "# Training parameters\n",
    "max_episodes = 5000\n",
    "max_timesteps = 2000\n",
    "save_interval = 100\n",
    "expl_noise = 0.25\n",
    "\n",
    "# Set random seeds (already set by seed_everything but keeping for clarity)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Create the agent\n",
    "kwargs = {\n",
    "    \"env_with_Dead\": env_with_Dead,\n",
    "    \"state_dim\": state_dim,\n",
    "    \"action_dim\": action_dim,\n",
    "    \"max_action\": max_action,\n",
    "    \"gamma\": 0.99,\n",
    "    \"net_width\": 200,\n",
    "    \"a_lr\": 1e-4,\n",
    "    \"c_lr\": 1e-4,\n",
    "    \"Q_batchsize\": 256,\n",
    "}\n",
    "\n",
    "agent = TD3(**kwargs)\n",
    "\n",
    "# Create replay buffer\n",
    "replay_buffer = ReplayBuffer.ReplayBuffer(state_dim, action_dim, max_size=int(1e6))\n",
    "\n",
    "# Track statistics for plotting\n",
    "tracker = rld.InfoTracker()\n",
    "all_ep_r = []\n",
    "\n",
    "# Training procedure\n",
    "for episode in range(max_episodes):\n",
    "    # Recording statistics and video can be switched on and off (video recording is slow!)\n",
    "    env.info = episode % 10 == 0  # track every x episodes\n",
    "    env.video = episode % 10 == 0  # record videos every x episodes\n",
    "    \n",
    "    # Reset for new episode\n",
    "    observation, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    # Decay exploration noise\n",
    "    expl_noise *= 0.999\n",
    "    \n",
    "    # Episode loop\n",
    "    for t in range(max_timesteps):\n",
    "        steps += 1\n",
    "        \n",
    "        # Select action with exploration noise\n",
    "        action = (\n",
    "            agent.select_action(observation) + \n",
    "            np.random.normal(0, max_action * expl_noise, size=action_dim)\n",
    "        ).clip(-max_action, max_action)\n",
    "        \n",
    "        # Take action in the environment\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Accumulate episode reward\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # Check whether done\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Special reward handling for Walker environments\n",
    "        if reward <= -100 and env_with_Dead:\n",
    "            reward = -1\n",
    "            replay_buffer.add(observation, action, reward, next_observation, True)\n",
    "        else:\n",
    "            replay_buffer.add(observation, action, reward, next_observation, done)\n",
    "        \n",
    "        # Update current observation\n",
    "        observation = next_observation\n",
    "        \n",
    "        # Train the agent if buffer has enough samples\n",
    "        if replay_buffer.size > 2000:\n",
    "            agent.train(replay_buffer)\n",
    "        \n",
    "        # Stop episode if done\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Save model at specified intervals\n",
    "    if (episode + 1) % save_interval == 0:\n",
    "        agent.save(episode + 1)\n",
    "    \n",
    "    # Record and log statistics\n",
    "    if episode == 0:\n",
    "        all_ep_r.append(episode_reward)\n",
    "    else:\n",
    "        all_ep_r.append(all_ep_r[-1] * 0.9 + episode_reward * 0.1)\n",
    "    \n",
    "    # Track and plot statistics\n",
    "    tracker.track(info)\n",
    "    \n",
    "    if (episode + 1) % 10 == 0:\n",
    "        tracker.plot(r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "        print('seed:', seed, 'episode:', episode, 'score:', episode_reward, \n",
    "                'step:', steps, 'smoothed score:', all_ep_r[-1])\n",
    "\n",
    "# Don't forget to close environment (e.g. triggers last video save)\n",
    "env.close()\n",
    "\n",
    "# Write log file (for coursework)\n",
    "env.write_log(folder=\"logs\", file=\"nkfn77-agent-log.txt\")  # replace with your username\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run testing after agent has finished training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To evaluate an existing agent:\n",
    "# eval_results, agent = run_all_evaluations()\n",
    "\n",
    "# To run ablation studies (time-intensive):\n",
    "# results = run_ablation_studies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small demo with a predefined heuristic that is suboptimal and has no notion of balance (and is designed for the orignal BipedalWalker environment)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.box2d.bipedal_walker import BipedalWalkerHeuristics\n",
    "\n",
    "env = rld.make(\n",
    "    \"rldurham/Walker\",\n",
    "    # \"BipedalWalker-v3\",\n",
    "    render_mode=\"human\",\n",
    "    # render_mode=\"rgb_array\",\n",
    "    hardcore=False,\n",
    "    # hardcore=True,\n",
    ")\n",
    "_, obs, info = rld.seed_everything(42, env)\n",
    "\n",
    "heuristics = BipedalWalkerHeuristics()\n",
    "\n",
    "act = heuristics.step_heuristic(obs)\n",
    "for _ in range(500):\n",
    "    obs, rew, terminated, truncated, info = env.step(act)\n",
    "    act = heuristics.step_heuristic(obs)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    if env.render_mode == \"rgb_array\":\n",
    "        rld.render(env, clear=True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
