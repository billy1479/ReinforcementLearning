{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTNU1mwGB1ZD"
   },
   "source": [
    "**Dependencies and imports**\n",
    "\n",
    "This can take a minute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install swig\n",
    "# !pip install --upgrade rldurham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import rldurham as rld\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJHtclV_30Re"
   },
   "source": [
    "**Reinforcement learning agent**\n",
    "\n",
    "Replace this with your own agent, I recommend starting with TD3 (lecture 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jXNHP8_U-rn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from collections import deque\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size=400):\n",
    "        \"\"\"\n",
    "        Initialize the Actor-Critic network with separate policy and value heads.\n",
    "        Uses larger networks and layer normalization for better stability.\n",
    "        \n",
    "        Args:\n",
    "            num_inputs (int): Dimension of state space\n",
    "            num_outputs (int): Dimension of action space\n",
    "            hidden_size (int): Number of neurons in hidden layers\n",
    "        \"\"\"\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # Shared feature extractor with layer normalization for better training stability\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Separate networks for policy and value to prevent interference\n",
    "        self.policy_network = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.LayerNorm(hidden_size // 2),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.value_network = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.LayerNorm(hidden_size // 2),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Policy head with smaller initialization for better initial exploration\n",
    "        self.policy_mean = nn.Linear(hidden_size // 2, num_outputs)\n",
    "        torch.nn.init.uniform_(self.policy_mean.weight, -1e-3, 1e-3)\n",
    "        torch.nn.init.uniform_(self.policy_mean.bias, -1e-3, 1e-3)\n",
    "        \n",
    "        # Value head\n",
    "        self.value = nn.Linear(hidden_size // 2, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass through the network with separate paths for policy and value\n",
    "        \n",
    "        Args:\n",
    "            state (torch.Tensor): Current state\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (action_mean, state_value)\n",
    "        \"\"\"\n",
    "        features = self.feature_extractor(state)\n",
    "        policy_features = self.policy_network(features)\n",
    "        value_features = self.value_network(features)\n",
    "        \n",
    "        action_mean = torch.tanh(self.policy_mean(policy_features))  # Constrain actions to [-1, 1]\n",
    "        state_value = self.value(value_features)\n",
    "        \n",
    "        return action_mean, state_value\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size=400, lr=2e-4, \n",
    "                 gamma=0.99, epsilon=0.2, value_coef=0.5, entropy_coef=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the PPO agent with optimized hyperparameters for BipedalWalker\n",
    "        \n",
    "        Args:\n",
    "            num_inputs (int): Dimension of state space\n",
    "            num_outputs (int): Dimension of action space\n",
    "            hidden_size (int): Number of neurons in hidden layers\n",
    "            lr (float): Learning rate\n",
    "            gamma (float): Discount factor\n",
    "            epsilon (float): PPO clipping parameter\n",
    "            value_coef (float): Value loss coefficient\n",
    "            entropy_coef (float): Entropy bonus coefficient\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.num_outputs = num_outputs\n",
    "        \n",
    "        # Initialize neural network and optimizer\n",
    "        self.ac_net = ActorCritic(num_inputs, num_outputs, hidden_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.ac_net.parameters(), lr=lr)\n",
    "        \n",
    "        # Initialize hyperparameters\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        # Initialize action standard deviation with more conservative initial value\n",
    "        self.action_std = 0.5\n",
    "        self.min_action_std = 0.05\n",
    "        self.action_std_decay_rate = 0.03\n",
    "        self.action_std_decay_freq = 200  # Decay frequency in episodes\n",
    "        \n",
    "        # Initialize memory with separate recent trajectory buffer\n",
    "        self.data = []\n",
    "        self.trajectory_length = 0\n",
    "        self.max_trajectory_length = 2048  # Maximum steps before forcing an update\n",
    "        \n",
    "        # Statistics tracking with longer history\n",
    "        self.rewards_history = deque(maxlen=200)\n",
    "        self.episode_count = 0\n",
    "        \n",
    "        # Store the most recent action and log probability for training\n",
    "        self.last_action = None\n",
    "        self.last_log_prob = None\n",
    "        \n",
    "        # Advantage normalization tracking\n",
    "        self.advantage_mean = 0\n",
    "        self.advantage_std = 1\n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        \"\"\"\n",
    "        Sample an action from the policy with noise reduction over time\n",
    "        \n",
    "        Args:\n",
    "            state (numpy.ndarray): Current state\n",
    "            \n",
    "        Returns:\n",
    "            list: List of action values as Python floats\n",
    "        \"\"\"\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_mean, _ = self.ac_net(state_tensor)\n",
    "            \n",
    "            # Add noise for exploration\n",
    "            std = torch.full_like(action_mean, self.action_std)\n",
    "            dist = Normal(action_mean, std)\n",
    "            action = dist.sample()\n",
    "            \n",
    "            # Calculate log probability for training\n",
    "            log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "            \n",
    "            # Convert to numpy and store for training\n",
    "            action_numpy = action.cpu().numpy().squeeze()\n",
    "            self.last_action = action_numpy\n",
    "            self.last_log_prob = log_prob.cpu().numpy()\n",
    "            \n",
    "            # Convert to list of Python floats\n",
    "            action_list = [float(a) for a in action_numpy]\n",
    "            \n",
    "            return action_list\n",
    "    \n",
    "    def put_data(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store transition in memory with reward scaling\n",
    "        \n",
    "        Args:\n",
    "            state (numpy.ndarray): Current state\n",
    "            action (list): Action taken (list of floats)\n",
    "            reward (float): Reward received\n",
    "            next_state (numpy.ndarray): Next state\n",
    "            done (bool): Whether episode is done\n",
    "        \"\"\"\n",
    "        # Scale reward for better training stability\n",
    "        scaled_reward = reward * 0.1  # Scale down rewards to prevent value function instability\n",
    "        \n",
    "        # Convert action list back to numpy array for storage\n",
    "        action_array = np.array(action, dtype=np.float32)\n",
    "        self.data.append((state, action_array, self.last_log_prob, scaled_reward, next_state, done))\n",
    "        \n",
    "        self.trajectory_length += 1\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the agent using PPO with several optimizations\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (policy_loss, value_loss)\n",
    "        \"\"\"\n",
    "        # Skip training if not enough data\n",
    "        if len(self.data) < 32:  # Reduced minimum batch size for early learning\n",
    "            return 0.0, 0.0\n",
    "            \n",
    "        # Adjust batch size based on available data\n",
    "        batch_size = min(256, max(32, len(self.data) // 4))\n",
    "            \n",
    "        # Convert stored data to tensors\n",
    "        states = torch.FloatTensor([t[0] for t in self.data]).to(self.device)\n",
    "        actions = torch.FloatTensor([t[1] for t in self.data]).to(self.device)\n",
    "        old_log_probs = torch.FloatTensor([t[2] for t in self.data]).to(self.device)\n",
    "        rewards = torch.FloatTensor([t[3] for t in self.data]).to(self.device)\n",
    "        next_states = torch.FloatTensor([t[4] for t in self.data]).to(self.device)\n",
    "        dones = torch.FloatTensor([t[5] for t in self.data]).to(self.device)\n",
    "        \n",
    "        # Compute returns and advantages with GAE\n",
    "        with torch.no_grad():\n",
    "            _, next_values = self.ac_net(next_states)\n",
    "            _, current_values = self.ac_net(states)\n",
    "            next_values = next_values.squeeze()\n",
    "            current_values = current_values.squeeze()\n",
    "            \n",
    "            # Calculate advantages using GAE\n",
    "            advantages = torch.zeros_like(rewards)\n",
    "            gae = 0\n",
    "            for t in reversed(range(len(rewards))):\n",
    "                if t == len(rewards) - 1:\n",
    "                    next_value = next_values[t]\n",
    "                else:\n",
    "                    next_value = current_values[t + 1]\n",
    "                    \n",
    "                delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - current_values[t]\n",
    "                gae = delta + self.gamma * 0.95 * (1 - dones[t]) * gae\n",
    "                advantages[t] = gae\n",
    "                \n",
    "            returns = advantages + current_values\n",
    "            \n",
    "            # Normalize advantages\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # PPO update with mini-batches\n",
    "        batch_size = min(256, len(self.data))\n",
    "        num_updates = 8  # Number of optimization epochs\n",
    "        \n",
    "        total_policy_loss = 0\n",
    "        total_value_loss = 0\n",
    "        \n",
    "        for _ in range(num_updates):\n",
    "            # Random sampling for mini-batches\n",
    "            indices = np.random.permutation(len(self.data))\n",
    "            \n",
    "            for start in range(0, len(self.data), batch_size):\n",
    "                end = start + batch_size\n",
    "                batch_indices = indices[start:end]\n",
    "                \n",
    "                # Get batch data\n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "                \n",
    "                # Get updated action probabilities and values\n",
    "                action_means, values = self.ac_net(batch_states)\n",
    "                dist = Normal(action_means, torch.full_like(action_means, self.action_std))\n",
    "                new_log_probs = dist.log_prob(batch_actions).sum(dim=-1)\n",
    "                \n",
    "                # Compute ratio and clipped ratio\n",
    "                ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "                clipped_ratio = torch.clamp(ratio, 1-self.epsilon, 1+self.epsilon)\n",
    "                \n",
    "                # Compute losses with additional penalty for large actions\n",
    "                policy_loss = -torch.min(ratio * batch_advantages, \n",
    "                                       clipped_ratio * batch_advantages).mean()\n",
    "                value_loss = ((values.squeeze() - batch_returns) ** 2).mean()\n",
    "                entropy_loss = -dist.entropy().mean()\n",
    "                \n",
    "                # Additional penalty for extreme actions\n",
    "                action_penalty = 0.1 * (torch.max(torch.abs(action_means)) - 0.9).clamp(min=0)\n",
    "                \n",
    "                # Combined loss\n",
    "                total_loss = (policy_loss + \n",
    "                             self.value_coef * value_loss + \n",
    "                             self.entropy_coef * entropy_loss +\n",
    "                             action_penalty)\n",
    "                \n",
    "                # Optimize\n",
    "                self.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.ac_net.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_policy_loss += policy_loss.item()\n",
    "                total_value_loss += value_loss.item()\n",
    "        \n",
    "        # Clear memory\n",
    "        self.data = []\n",
    "        self.trajectory_length = 0\n",
    "        \n",
    "        # Calculate number of actual updates performed\n",
    "        num_batches = max(1, len(self.data) // batch_size)\n",
    "        total_updates = num_updates * num_batches\n",
    "        \n",
    "        # Return average losses, protecting against division by zero\n",
    "        if total_updates > 0:\n",
    "            avg_policy_loss = total_policy_loss / total_updates\n",
    "            avg_value_loss = total_value_loss / total_updates\n",
    "        else:\n",
    "            avg_policy_loss = total_policy_loss\n",
    "            avg_value_loss = total_value_loss\n",
    "        \n",
    "        return avg_policy_loss, avg_value_loss\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset episode-specific parameters\"\"\"\n",
    "        self.last_action = None\n",
    "        self.last_log_prob = None\n",
    "        \n",
    "        # Force training if trajectory is too long\n",
    "        if self.trajectory_length >= self.max_trajectory_length:\n",
    "            self.train()\n",
    "            self.trajectory_length = 0\n",
    "    \n",
    "    def update_stats(self, episode_reward):\n",
    "        \"\"\"\n",
    "        Update training statistics and adjust exploration\n",
    "        \n",
    "        Args:\n",
    "            episode_reward (float): Total reward for the episode\n",
    "        \"\"\"\n",
    "        self.rewards_history.append(episode_reward)\n",
    "        self.episode_count += 1\n",
    "        \n",
    "        # More conservative action std decay\n",
    "        if self.episode_count % self.action_std_decay_freq == 0:\n",
    "            # Only decay if we're seeing improvement\n",
    "            if np.mean(list(self.rewards_history)[-20:]) > -100:\n",
    "                self.action_std = max(self.min_action_std, \n",
    "                                    self.action_std - self.action_std_decay_rate)\n",
    "    \n",
    "    def get_average_reward(self):\n",
    "        \"\"\"\n",
    "        Get average reward over last 100 episodes\n",
    "        \n",
    "        Returns:\n",
    "            float: Average reward\n",
    "        \"\"\"\n",
    "        return np.mean(self.rewards_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEv4ZjXmyrHo"
   },
   "source": [
    "**Prepare the environment and wrap it to capture statistics, logs, and videos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Xrcek4hxDXl"
   },
   "outputs": [],
   "source": [
    "env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\")\n",
    "# env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\", hardcore=True) # only attempt this when your agent has solved the non-hardcore version\n",
    "\n",
    "# get statistics, logs, and videos\n",
    "env = rld.Recorder(\n",
    "    env,\n",
    "    smoothing=10,                       # track rolling averages (useful for plotting)\n",
    "    video=True,                         # enable recording videos\n",
    "    video_folder=\"videos\",              # folder for videos\n",
    "    video_prefix=\"xxxx00-agent-video\",  # prefix for videos (replace xxxx00 with your username)\n",
    "    logs=True,                          # keep logs\n",
    ")\n",
    "\n",
    "# training on CPU recommended\n",
    "rld.check_device()\n",
    "\n",
    "# environment info\n",
    "discrete_act, discrete_obs, act_dim, obs_dim = rld.env_info(env, print_out=True)\n",
    "\n",
    "# render start image\n",
    "env.reset(seed=42)\n",
    "rld.render(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "rDl6ViIDlVOk",
    "outputId": "731e4ce7-c98d-4bde-8a2c-fbdf1410e24f"
   },
   "outputs": [],
   "source": [
    "# in the submission please use seed_everything with seed 42 for verification\n",
    "seed, observation, info = rld.seed_everything(42, env)\n",
    "\n",
    "# initialise agent\n",
    "agent = PPOAgent(env.observation_space.shape[0], env.action_space.shape[0])\n",
    "max_episodes = 1000\n",
    "max_timesteps = 2000\n",
    "\n",
    "# track statistics for plotting\n",
    "tracker = rld.InfoTracker()\n",
    "\n",
    "# switch video recording off (only switch on every x episodes as this is slow)\n",
    "env.video = False\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    # Set up episode\n",
    "    env.info = episode % 10 == 0\n",
    "    env.video = episode % 10 == 0\n",
    "    current_observation, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    # Reset agent's temporal smoothing\n",
    "    agent.reset()\n",
    "    \n",
    "    # Run episode\n",
    "    for t in range(max_timesteps):\n",
    "        # Get action from agent\n",
    "        action = agent.sample_action(current_observation)\n",
    "        \n",
    "        # Take step in environment\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Store transition and update episode reward\n",
    "        agent.put_data(current_observation, action, reward, next_observation, \n",
    "                      terminated or truncated)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # Update observation\n",
    "        current_observation = next_observation\n",
    "        \n",
    "        # Check if episode is done\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    # Train agent and update statistics\n",
    "    policy_loss, value_loss = agent.train()\n",
    "    agent.update_stats(episode_reward)\n",
    "    \n",
    "    # Print progress every 10 episodes\n",
    "    if episode % 10 == 0:\n",
    "        avg_reward = agent.get_average_reward()\n",
    "        print(f\"Episode {episode+1}, Reward: {episode_reward:.2f}, \"\n",
    "              f\"Avg Reward: {avg_reward:.2f}, \"\n",
    "              f\"Action STD: {agent.action_std:.3f}\")\n",
    "    \n",
    "    # Track and plot\n",
    "    tracker.track(info)\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        tracker.plot(r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Write log file\n",
    "env.write_log(folder=\"logs\", file=\"xxxx00-agent-log.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small demo with a predefined heuristic that is suboptimal and has no notion of balance (and is designed for the orignal BipedalWalker environment)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.box2d.bipedal_walker import BipedalWalkerHeuristics\n",
    "\n",
    "env = rld.make(\n",
    "    \"rldurham/Walker\",\n",
    "    # \"BipedalWalker-v3\",\n",
    "    render_mode=\"human\",\n",
    "    # render_mode=\"rgb_array\",\n",
    "    hardcore=False,\n",
    "    # hardcore=True,\n",
    ")\n",
    "_, obs, info = rld.seed_everything(42, env)\n",
    "\n",
    "heuristics = BipedalWalkerHeuristics()\n",
    "\n",
    "act = heuristics.step_heuristic(obs)\n",
    "for _ in range(500):\n",
    "    obs, rew, terminated, truncated, info = env.step(act)\n",
    "    act = heuristics.step_heuristic(obs)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    if env.render_mode == \"rgb_array\":\n",
    "        rld.render(env, clear=True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
